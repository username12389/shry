{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 16:20:17,047 - INFO - Starting IV Crush Analysis...\n",
      "2025-02-13 16:20:18,667 - INFO - Initialized analyzer with 503 symbols\n",
      "2025-02-13 16:20:18,669 - INFO - Analyzing S&P 500 stocks for IV crush probability...\n",
      "2025-02-13 16:20:18,671 - INFO - \n",
      "Analyzing MMM...\n",
      "2025-02-13 16:20:18,673 - INFO - Fetching data for MMM\n",
      "2025-02-13 16:20:20,336 - INFO - Fetching 2025-02-14 options for MMM\n",
      "2025-02-13 16:20:20,541 - INFO - Successfully fetched 2025-02-14 options for MMM\n",
      "2025-02-13 16:20:20,543 - INFO - Fetching 2025-02-21 options for MMM\n",
      "2025-02-13 16:20:20,749 - INFO - Successfully fetched 2025-02-21 options for MMM\n",
      "2025-02-13 16:20:20,751 - INFO - Fetching 2025-02-28 options for MMM\n",
      "2025-02-13 16:20:20,960 - INFO - Successfully fetched 2025-02-28 options for MMM\n",
      "2025-02-13 16:20:20,962 - INFO - Fetching 2025-03-07 options for MMM\n",
      "2025-02-13 16:20:21,181 - INFO - Successfully fetched 2025-03-07 options for MMM\n",
      "2025-02-13 16:20:21,183 - INFO - Cached data for MMM\n",
      "2025-02-13 16:20:21,826 - INFO - Using cached data for MMM (age: 3.2s)\n",
      "2025-02-13 16:20:21,834 - INFO - Successfully analyzed MMM in 3.16 seconds\n",
      "2025-02-13 16:20:21,835 - INFO - Using cached data for MMM (age: 3.2s)\n",
      "2025-02-13 16:20:21,871 - INFO - \n",
      "Analyzing AOS...\n",
      "2025-02-13 16:20:21,874 - INFO - Fetching data for AOS\n",
      "2025-02-13 16:20:22,069 - INFO - Fetching 2025-02-21 options for AOS\n",
      "2025-02-13 16:20:22,267 - INFO - Successfully fetched 2025-02-21 options for AOS\n",
      "2025-02-13 16:20:22,269 - INFO - Fetching 2025-03-21 options for AOS\n",
      "2025-02-13 16:20:22,540 - INFO - Successfully fetched 2025-03-21 options for AOS\n",
      "2025-02-13 16:20:22,543 - INFO - Fetching 2025-04-17 options for AOS\n",
      "2025-02-13 16:20:22,751 - INFO - Successfully fetched 2025-04-17 options for AOS\n",
      "2025-02-13 16:20:22,752 - INFO - Fetching 2025-07-18 options for AOS\n",
      "2025-02-13 16:20:22,958 - INFO - Successfully fetched 2025-07-18 options for AOS\n",
      "2025-02-13 16:20:22,959 - INFO - Cached data for AOS\n",
      "2025-02-13 16:20:23,216 - INFO - Using cached data for AOS (age: 1.3s)\n",
      "2025-02-13 16:20:23,229 - INFO - Successfully analyzed AOS in 1.36 seconds\n",
      "2025-02-13 16:20:23,230 - INFO - Using cached data for AOS (age: 1.4s)\n",
      "2025-02-13 16:20:23,267 - INFO - \n",
      "Analyzing ABT...\n",
      "2025-02-13 16:20:23,272 - INFO - Fetching data for ABT\n",
      "2025-02-13 16:20:23,479 - INFO - Fetching 2025-02-14 options for ABT\n",
      "2025-02-13 16:20:23,690 - INFO - Successfully fetched 2025-02-14 options for ABT\n",
      "2025-02-13 16:20:23,691 - INFO - Fetching 2025-02-21 options for ABT\n",
      "2025-02-13 16:20:23,907 - INFO - Successfully fetched 2025-02-21 options for ABT\n",
      "2025-02-13 16:20:23,909 - INFO - Fetching 2025-02-28 options for ABT\n",
      "2025-02-13 16:20:24,103 - INFO - Successfully fetched 2025-02-28 options for ABT\n",
      "2025-02-13 16:20:24,105 - INFO - Fetching 2025-03-07 options for ABT\n",
      "2025-02-13 16:20:24,308 - INFO - Successfully fetched 2025-03-07 options for ABT\n",
      "2025-02-13 16:20:24,310 - INFO - Cached data for ABT\n",
      "2025-02-13 16:20:24,564 - INFO - Using cached data for ABT (age: 1.3s)\n",
      "2025-02-13 16:20:24,572 - INFO - Successfully analyzed ABT in 1.30 seconds\n",
      "2025-02-13 16:20:24,573 - INFO - Using cached data for ABT (age: 1.3s)\n",
      "2025-02-13 16:20:24,594 - INFO - \n",
      "Analyzing ABBV...\n",
      "2025-02-13 16:20:24,597 - INFO - Fetching data for ABBV\n",
      "2025-02-13 16:20:24,793 - INFO - Fetching 2025-02-14 options for ABBV\n",
      "2025-02-13 16:20:25,003 - INFO - Successfully fetched 2025-02-14 options for ABBV\n",
      "2025-02-13 16:20:25,011 - INFO - Fetching 2025-02-21 options for ABBV\n",
      "2025-02-13 16:20:25,285 - INFO - Successfully fetched 2025-02-21 options for ABBV\n",
      "2025-02-13 16:20:25,288 - INFO - Fetching 2025-02-28 options for ABBV\n",
      "2025-02-13 16:20:25,553 - INFO - Successfully fetched 2025-02-28 options for ABBV\n",
      "2025-02-13 16:20:25,555 - INFO - Fetching 2025-03-07 options for ABBV\n",
      "2025-02-13 16:20:25,752 - INFO - Successfully fetched 2025-03-07 options for ABBV\n",
      "2025-02-13 16:20:25,754 - INFO - Cached data for ABBV\n",
      "2025-02-13 16:20:25,997 - INFO - Using cached data for ABBV (age: 1.4s)\n",
      "2025-02-13 16:20:26,012 - INFO - Successfully analyzed ABBV in 1.42 seconds\n",
      "2025-02-13 16:20:26,014 - INFO - Using cached data for ABBV (age: 1.4s)\n",
      "2025-02-13 16:20:26,034 - INFO - \n",
      "Analyzing ACN...\n",
      "2025-02-13 16:20:26,036 - INFO - Fetching data for ACN\n",
      "2025-02-13 16:20:26,237 - INFO - Fetching 2025-02-14 options for ACN\n",
      "2025-02-13 16:20:26,450 - INFO - Successfully fetched 2025-02-14 options for ACN\n",
      "2025-02-13 16:20:26,452 - INFO - Fetching 2025-02-21 options for ACN\n",
      "2025-02-13 16:20:26,659 - INFO - Successfully fetched 2025-02-21 options for ACN\n",
      "2025-02-13 16:20:26,662 - INFO - Fetching 2025-02-28 options for ACN\n",
      "2025-02-13 16:20:26,864 - INFO - Successfully fetched 2025-02-28 options for ACN\n",
      "2025-02-13 16:20:26,867 - INFO - Fetching 2025-03-07 options for ACN\n",
      "2025-02-13 16:20:27,073 - INFO - Successfully fetched 2025-03-07 options for ACN\n",
      "2025-02-13 16:20:27,075 - INFO - Cached data for ACN\n",
      "2025-02-13 16:20:27,305 - INFO - Using cached data for ACN (age: 1.3s)\n",
      "2025-02-13 16:20:27,310 - INFO - Successfully analyzed ACN in 1.27 seconds\n",
      "2025-02-13 16:20:27,311 - INFO - Using cached data for ACN (age: 1.3s)\n",
      "2025-02-13 16:20:27,320 - INFO - \n",
      "Analyzing ADBE...\n",
      "2025-02-13 16:20:27,321 - INFO - Fetching data for ADBE\n",
      "2025-02-13 16:20:27,522 - INFO - Fetching 2025-02-14 options for ADBE\n",
      "2025-02-13 16:20:27,727 - INFO - Successfully fetched 2025-02-14 options for ADBE\n",
      "2025-02-13 16:20:27,732 - INFO - Fetching 2025-02-21 options for ADBE\n",
      "2025-02-13 16:20:27,949 - INFO - Successfully fetched 2025-02-21 options for ADBE\n",
      "2025-02-13 16:20:27,952 - INFO - Fetching 2025-02-28 options for ADBE\n",
      "2025-02-13 16:20:28,159 - INFO - Successfully fetched 2025-02-28 options for ADBE\n",
      "2025-02-13 16:20:28,160 - INFO - Fetching 2025-03-07 options for ADBE\n",
      "2025-02-13 16:20:28,360 - INFO - Successfully fetched 2025-03-07 options for ADBE\n",
      "2025-02-13 16:20:28,361 - INFO - Cached data for ADBE\n",
      "2025-02-13 16:20:28,592 - INFO - Using cached data for ADBE (age: 1.3s)\n",
      "2025-02-13 16:20:28,603 - INFO - Successfully analyzed ADBE in 1.28 seconds\n",
      "2025-02-13 16:20:28,606 - INFO - Using cached data for ADBE (age: 1.3s)\n",
      "2025-02-13 16:20:28,635 - INFO - \n",
      "Analyzing AMD...\n",
      "2025-02-13 16:20:28,638 - INFO - Fetching data for AMD\n",
      "2025-02-13 16:20:28,851 - INFO - Fetching 2025-02-14 options for AMD\n",
      "2025-02-13 16:20:29,073 - INFO - Successfully fetched 2025-02-14 options for AMD\n",
      "2025-02-13 16:20:29,075 - INFO - Fetching 2025-02-21 options for AMD\n",
      "2025-02-13 16:20:29,291 - INFO - Successfully fetched 2025-02-21 options for AMD\n",
      "2025-02-13 16:20:29,292 - INFO - Fetching 2025-02-28 options for AMD\n",
      "2025-02-13 16:20:29,512 - INFO - Successfully fetched 2025-02-28 options for AMD\n",
      "2025-02-13 16:20:29,515 - INFO - Fetching 2025-03-07 options for AMD\n",
      "2025-02-13 16:20:29,722 - INFO - Successfully fetched 2025-03-07 options for AMD\n",
      "2025-02-13 16:20:29,723 - INFO - Cached data for AMD\n",
      "2025-02-13 16:20:29,928 - INFO - Using cached data for AMD (age: 1.3s)\n",
      "2025-02-13 16:20:29,936 - INFO - Successfully analyzed AMD in 1.30 seconds\n",
      "2025-02-13 16:20:29,937 - INFO - Using cached data for AMD (age: 1.3s)\n",
      "2025-02-13 16:20:29,954 - INFO - \n",
      "Analyzing AES...\n",
      "2025-02-13 16:20:29,955 - INFO - Fetching data for AES\n",
      "2025-02-13 16:20:30,154 - INFO - Fetching 2025-02-21 options for AES\n",
      "2025-02-13 16:20:30,412 - INFO - Successfully fetched 2025-02-21 options for AES\n",
      "2025-02-13 16:20:30,415 - INFO - Fetching 2025-03-21 options for AES\n",
      "2025-02-13 16:20:30,628 - INFO - Successfully fetched 2025-03-21 options for AES\n",
      "2025-02-13 16:20:30,635 - INFO - Fetching 2025-05-16 options for AES\n",
      "2025-02-13 16:20:30,849 - INFO - Successfully fetched 2025-05-16 options for AES\n",
      "2025-02-13 16:20:30,851 - INFO - Fetching 2025-06-20 options for AES\n",
      "2025-02-13 16:20:31,052 - INFO - Successfully fetched 2025-06-20 options for AES\n",
      "2025-02-13 16:20:31,053 - INFO - Cached data for AES\n",
      "2025-02-13 16:20:31,307 - INFO - Using cached data for AES (age: 1.4s)\n",
      "2025-02-13 16:20:31,321 - INFO - Successfully analyzed AES in 1.37 seconds\n",
      "2025-02-13 16:20:31,322 - INFO - Using cached data for AES (age: 1.4s)\n",
      "2025-02-13 16:20:31,343 - INFO - \n",
      "Analyzing AFL...\n",
      "2025-02-13 16:20:31,345 - INFO - Fetching data for AFL\n",
      "2025-02-13 16:20:31,603 - INFO - Fetching 2025-02-14 options for AFL\n",
      "2025-02-13 16:20:31,826 - INFO - Successfully fetched 2025-02-14 options for AFL\n",
      "2025-02-13 16:20:31,830 - INFO - Fetching 2025-02-21 options for AFL\n",
      "2025-02-13 16:20:32,039 - INFO - Successfully fetched 2025-02-21 options for AFL\n",
      "2025-02-13 16:20:32,041 - INFO - Fetching 2025-02-28 options for AFL\n",
      "2025-02-13 16:20:32,254 - INFO - Successfully fetched 2025-02-28 options for AFL\n",
      "2025-02-13 16:20:32,255 - INFO - Fetching 2025-03-07 options for AFL\n",
      "2025-02-13 16:20:32,480 - INFO - Successfully fetched 2025-03-07 options for AFL\n",
      "2025-02-13 16:20:32,483 - INFO - Cached data for AFL\n",
      "2025-02-13 16:20:32,819 - INFO - Using cached data for AFL (age: 1.5s)\n",
      "2025-02-13 16:20:32,827 - INFO - Successfully analyzed AFL in 1.48 seconds\n",
      "2025-02-13 16:20:32,828 - INFO - Using cached data for AFL (age: 1.5s)\n",
      "2025-02-13 16:20:32,857 - INFO - \n",
      "Analyzing A...\n",
      "2025-02-13 16:20:32,862 - INFO - Fetching data for A\n",
      "2025-02-13 16:20:33,119 - INFO - Fetching 2025-02-21 options for A\n",
      "2025-02-13 16:20:33,391 - INFO - Successfully fetched 2025-02-21 options for A\n",
      "2025-02-13 16:20:33,393 - INFO - Fetching 2025-03-21 options for A\n",
      "2025-02-13 16:20:33,601 - INFO - Successfully fetched 2025-03-21 options for A\n",
      "2025-02-13 16:20:33,604 - INFO - Fetching 2025-04-17 options for A\n",
      "2025-02-13 16:20:33,806 - INFO - Successfully fetched 2025-04-17 options for A\n",
      "2025-02-13 16:20:33,807 - INFO - Fetching 2025-05-16 options for A\n",
      "2025-02-13 16:20:34,017 - INFO - Successfully fetched 2025-05-16 options for A\n",
      "2025-02-13 16:20:34,019 - INFO - Cached data for A\n",
      "2025-02-13 16:20:34,281 - INFO - Using cached data for A (age: 1.4s)\n",
      "2025-02-13 16:20:34,292 - INFO - Successfully analyzed A in 1.43 seconds\n",
      "2025-02-13 16:20:34,294 - INFO - Using cached data for A (age: 1.4s)\n",
      "2025-02-13 16:20:34,320 - INFO - \n",
      "Analyzing APD...\n",
      "2025-02-13 16:20:34,322 - INFO - Fetching data for APD\n",
      "2025-02-13 16:20:34,516 - INFO - Fetching 2025-02-21 options for APD\n",
      "2025-02-13 16:20:34,729 - INFO - Successfully fetched 2025-02-21 options for APD\n",
      "2025-02-13 16:20:34,734 - INFO - Fetching 2025-03-21 options for APD\n",
      "2025-02-13 16:20:34,999 - INFO - Successfully fetched 2025-03-21 options for APD\n",
      "2025-02-13 16:20:35,002 - INFO - Fetching 2025-04-17 options for APD\n",
      "2025-02-13 16:20:35,203 - INFO - Successfully fetched 2025-04-17 options for APD\n",
      "2025-02-13 16:20:35,205 - INFO - Fetching 2025-06-20 options for APD\n",
      "2025-02-13 16:20:35,447 - INFO - Successfully fetched 2025-06-20 options for APD\n",
      "2025-02-13 16:20:35,449 - INFO - Cached data for APD\n",
      "2025-02-13 16:20:35,754 - INFO - Using cached data for APD (age: 1.4s)\n",
      "2025-02-13 16:20:35,764 - INFO - Successfully analyzed APD in 1.44 seconds\n",
      "2025-02-13 16:20:35,765 - INFO - Using cached data for APD (age: 1.4s)\n",
      "2025-02-13 16:20:35,795 - INFO - \n",
      "Analyzing ABNB...\n",
      "2025-02-13 16:20:35,798 - INFO - Fetching data for ABNB\n",
      "2025-02-13 16:20:36,007 - INFO - Fetching 2025-02-14 options for ABNB\n",
      "2025-02-13 16:20:36,217 - INFO - Successfully fetched 2025-02-14 options for ABNB\n",
      "2025-02-13 16:20:36,219 - INFO - Fetching 2025-02-21 options for ABNB\n",
      "2025-02-13 16:20:36,438 - INFO - Successfully fetched 2025-02-21 options for ABNB\n",
      "2025-02-13 16:20:36,440 - INFO - Fetching 2025-02-28 options for ABNB\n",
      "2025-02-13 16:20:36,647 - INFO - Successfully fetched 2025-02-28 options for ABNB\n",
      "2025-02-13 16:20:36,650 - INFO - Fetching 2025-03-07 options for ABNB\n",
      "2025-02-13 16:20:36,861 - INFO - Successfully fetched 2025-03-07 options for ABNB\n",
      "2025-02-13 16:20:36,863 - INFO - Cached data for ABNB\n",
      "2025-02-13 16:20:37,099 - INFO - Using cached data for ABNB (age: 1.3s)\n",
      "2025-02-13 16:20:37,104 - INFO - Successfully analyzed ABNB in 1.31 seconds\n",
      "2025-02-13 16:20:37,105 - INFO - Using cached data for ABNB (age: 1.3s)\n",
      "2025-02-13 16:20:37,116 - INFO - \n",
      "Analyzing AKAM...\n",
      "2025-02-13 16:20:37,117 - INFO - Fetching data for AKAM\n",
      "2025-02-13 16:20:37,304 - INFO - Fetching 2025-02-14 options for AKAM\n",
      "2025-02-13 16:20:37,547 - INFO - Successfully fetched 2025-02-14 options for AKAM\n",
      "2025-02-13 16:20:37,551 - INFO - Fetching 2025-02-21 options for AKAM\n",
      "2025-02-13 16:20:37,756 - INFO - Successfully fetched 2025-02-21 options for AKAM\n",
      "2025-02-13 16:20:37,758 - INFO - Fetching 2025-02-28 options for AKAM\n",
      "2025-02-13 16:20:38,039 - INFO - Successfully fetched 2025-02-28 options for AKAM\n",
      "2025-02-13 16:20:38,041 - INFO - Fetching 2025-03-07 options for AKAM\n",
      "2025-02-13 16:20:38,495 - INFO - Successfully fetched 2025-03-07 options for AKAM\n",
      "2025-02-13 16:20:38,498 - INFO - Cached data for AKAM\n",
      "2025-02-13 16:20:38,720 - INFO - Using cached data for AKAM (age: 1.6s)\n",
      "2025-02-13 16:20:38,730 - INFO - Successfully analyzed AKAM in 1.61 seconds\n",
      "2025-02-13 16:20:38,731 - INFO - Using cached data for AKAM (age: 1.6s)\n",
      "2025-02-13 16:20:38,771 - INFO - \n",
      "Analyzing ALB...\n",
      "2025-02-13 16:20:38,774 - INFO - Fetching data for ALB\n",
      "2025-02-13 16:20:38,969 - INFO - Fetching 2025-02-14 options for ALB\n",
      "2025-02-13 16:20:39,174 - INFO - Successfully fetched 2025-02-14 options for ALB\n",
      "2025-02-13 16:20:39,176 - INFO - Fetching 2025-02-21 options for ALB\n",
      "2025-02-13 16:20:39,386 - INFO - Successfully fetched 2025-02-21 options for ALB\n",
      "2025-02-13 16:20:39,388 - INFO - Fetching 2025-02-28 options for ALB\n",
      "2025-02-13 16:20:39,656 - INFO - Successfully fetched 2025-02-28 options for ALB\n",
      "2025-02-13 16:20:39,658 - INFO - Fetching 2025-03-07 options for ALB\n",
      "2025-02-13 16:20:39,862 - INFO - Successfully fetched 2025-03-07 options for ALB\n",
      "2025-02-13 16:20:39,863 - INFO - Cached data for ALB\n",
      "2025-02-13 16:20:40,136 - INFO - Using cached data for ALB (age: 1.4s)\n",
      "2025-02-13 16:20:40,180 - INFO - Successfully analyzed ALB in 1.41 seconds\n",
      "2025-02-13 16:20:40,186 - INFO - Using cached data for ALB (age: 1.4s)\n",
      "2025-02-13 16:20:40,250 - INFO - \n",
      "Analyzing ARE...\n",
      "2025-02-13 16:20:40,254 - INFO - Fetching data for ARE\n",
      "2025-02-13 16:20:40,457 - INFO - Fetching 2025-02-21 options for ARE\n",
      "2025-02-13 16:20:40,658 - INFO - Successfully fetched 2025-02-21 options for ARE\n",
      "2025-02-13 16:20:40,660 - INFO - Fetching 2025-03-21 options for ARE\n",
      "2025-02-13 16:20:40,857 - INFO - Successfully fetched 2025-03-21 options for ARE\n",
      "2025-02-13 16:20:40,859 - INFO - Fetching 2025-04-17 options for ARE\n",
      "2025-02-13 16:20:41,066 - INFO - Successfully fetched 2025-04-17 options for ARE\n",
      "2025-02-13 16:20:41,068 - INFO - Fetching 2025-07-18 options for ARE\n",
      "2025-02-13 16:20:41,272 - INFO - Successfully fetched 2025-07-18 options for ARE\n",
      "2025-02-13 16:20:41,274 - INFO - Cached data for ARE\n",
      "2025-02-13 16:20:41,534 - INFO - Using cached data for ARE (age: 1.3s)\n",
      "2025-02-13 16:20:41,543 - INFO - Successfully analyzed ARE in 1.29 seconds\n",
      "2025-02-13 16:20:41,544 - INFO - Using cached data for ARE (age: 1.3s)\n",
      "2025-02-13 16:20:41,570 - INFO - \n",
      "Analyzing ALGN...\n",
      "2025-02-13 16:20:41,574 - INFO - Fetching data for ALGN\n",
      "2025-02-13 16:20:41,789 - INFO - Fetching 2025-02-14 options for ALGN\n",
      "2025-02-13 16:20:41,994 - INFO - Successfully fetched 2025-02-14 options for ALGN\n",
      "2025-02-13 16:20:41,996 - INFO - Fetching 2025-02-21 options for ALGN\n",
      "2025-02-13 16:20:42,235 - INFO - Successfully fetched 2025-02-21 options for ALGN\n",
      "2025-02-13 16:20:42,237 - INFO - Fetching 2025-02-28 options for ALGN\n",
      "2025-02-13 16:20:42,440 - INFO - Successfully fetched 2025-02-28 options for ALGN\n",
      "2025-02-13 16:20:42,442 - INFO - Fetching 2025-03-07 options for ALGN\n",
      "2025-02-13 16:20:42,641 - INFO - Successfully fetched 2025-03-07 options for ALGN\n",
      "2025-02-13 16:20:42,644 - INFO - Cached data for ALGN\n",
      "2025-02-13 16:20:42,863 - INFO - Using cached data for ALGN (age: 1.3s)\n",
      "2025-02-13 16:20:42,874 - INFO - Successfully analyzed ALGN in 1.30 seconds\n",
      "2025-02-13 16:20:42,875 - INFO - Using cached data for ALGN (age: 1.3s)\n",
      "2025-02-13 16:20:42,910 - INFO - \n",
      "Analyzing ALLE...\n",
      "2025-02-13 16:20:42,913 - INFO - Fetching data for ALLE\n",
      "2025-02-13 16:20:43,118 - INFO - Fetching 2025-02-21 options for ALLE\n",
      "2025-02-13 16:20:43,322 - INFO - Successfully fetched 2025-02-21 options for ALLE\n",
      "2025-02-13 16:20:43,325 - INFO - Fetching 2025-03-21 options for ALLE\n",
      "2025-02-13 16:20:43,523 - INFO - Successfully fetched 2025-03-21 options for ALLE\n",
      "2025-02-13 16:20:43,525 - INFO - Fetching 2025-06-20 options for ALLE\n",
      "2025-02-13 16:20:43,774 - INFO - Successfully fetched 2025-06-20 options for ALLE\n",
      "2025-02-13 16:20:43,777 - INFO - Fetching 2025-09-19 options for ALLE\n",
      "2025-02-13 16:20:43,979 - INFO - Successfully fetched 2025-09-19 options for ALLE\n",
      "2025-02-13 16:20:43,982 - INFO - Cached data for ALLE\n",
      "2025-02-13 16:20:44,236 - INFO - Using cached data for ALLE (age: 1.3s)\n",
      "2025-02-13 16:20:44,265 - INFO - Successfully analyzed ALLE in 1.35 seconds\n",
      "2025-02-13 16:20:44,267 - INFO - Using cached data for ALLE (age: 1.4s)\n",
      "2025-02-13 16:20:44,294 - INFO - \n",
      "Analyzing LNT...\n",
      "2025-02-13 16:20:44,300 - INFO - Fetching data for LNT\n",
      "2025-02-13 16:20:44,498 - INFO - Fetching 2025-02-21 options for LNT\n",
      "2025-02-13 16:20:44,706 - INFO - Successfully fetched 2025-02-21 options for LNT\n",
      "2025-02-13 16:20:44,709 - INFO - Fetching 2025-03-21 options for LNT\n",
      "2025-02-13 16:20:44,927 - INFO - Successfully fetched 2025-03-21 options for LNT\n",
      "2025-02-13 16:20:44,931 - INFO - Fetching 2025-04-17 options for LNT\n",
      "2025-02-13 16:20:45,141 - INFO - Successfully fetched 2025-04-17 options for LNT\n",
      "2025-02-13 16:20:45,143 - INFO - Fetching 2025-07-18 options for LNT\n",
      "2025-02-13 16:20:45,351 - INFO - Successfully fetched 2025-07-18 options for LNT\n",
      "2025-02-13 16:20:45,353 - INFO - Cached data for LNT\n",
      "2025-02-13 16:20:45,646 - INFO - Using cached data for LNT (age: 1.3s)\n",
      "2025-02-13 16:20:45,658 - INFO - Successfully analyzed LNT in 1.36 seconds\n",
      "2025-02-13 16:20:45,659 - INFO - Using cached data for LNT (age: 1.4s)\n",
      "2025-02-13 16:20:45,686 - INFO - \n",
      "Analyzing ALL...\n",
      "2025-02-13 16:20:45,689 - INFO - Fetching data for ALL\n",
      "2025-02-13 16:20:45,893 - INFO - Fetching 2025-02-21 options for ALL\n",
      "2025-02-13 16:20:46,097 - INFO - Successfully fetched 2025-02-21 options for ALL\n",
      "2025-02-13 16:20:46,100 - INFO - Fetching 2025-03-21 options for ALL\n",
      "2025-02-13 16:20:46,310 - INFO - Successfully fetched 2025-03-21 options for ALL\n",
      "2025-02-13 16:20:46,315 - INFO - Fetching 2025-04-17 options for ALL\n",
      "2025-02-13 16:20:46,525 - INFO - Successfully fetched 2025-04-17 options for ALL\n",
      "2025-02-13 16:20:46,526 - INFO - Fetching 2025-06-20 options for ALL\n",
      "2025-02-13 16:20:46,738 - INFO - Successfully fetched 2025-06-20 options for ALL\n",
      "2025-02-13 16:20:46,739 - INFO - Cached data for ALL\n",
      "2025-02-13 16:20:47,003 - INFO - Using cached data for ALL (age: 1.3s)\n",
      "2025-02-13 16:20:47,019 - INFO - Successfully analyzed ALL in 1.33 seconds\n",
      "2025-02-13 16:20:47,022 - INFO - Using cached data for ALL (age: 1.3s)\n",
      "2025-02-13 16:20:47,063 - INFO - \n",
      "Analyzing GOOGL...\n",
      "2025-02-13 16:20:47,068 - INFO - Fetching data for GOOGL\n",
      "2025-02-13 16:20:47,295 - INFO - Fetching 2025-02-14 options for GOOGL\n",
      "2025-02-13 16:20:47,509 - INFO - Successfully fetched 2025-02-14 options for GOOGL\n",
      "2025-02-13 16:20:47,511 - INFO - Fetching 2025-02-21 options for GOOGL\n",
      "2025-02-13 16:20:47,762 - INFO - Successfully fetched 2025-02-21 options for GOOGL\n",
      "2025-02-13 16:20:47,764 - INFO - Fetching 2025-02-28 options for GOOGL\n",
      "2025-02-13 16:20:47,985 - INFO - Successfully fetched 2025-02-28 options for GOOGL\n",
      "2025-02-13 16:20:47,987 - INFO - Fetching 2025-03-07 options for GOOGL\n",
      "2025-02-13 16:20:48,222 - INFO - Successfully fetched 2025-03-07 options for GOOGL\n",
      "2025-02-13 16:20:48,224 - INFO - Cached data for GOOGL\n",
      "2025-02-13 16:20:48,460 - INFO - Using cached data for GOOGL (age: 1.4s)\n",
      "2025-02-13 16:20:48,473 - INFO - Successfully analyzed GOOGL in 1.41 seconds\n",
      "2025-02-13 16:20:48,474 - INFO - Using cached data for GOOGL (age: 1.4s)\n",
      "2025-02-13 16:20:48,510 - INFO - \n",
      "Analyzing GOOG...\n",
      "2025-02-13 16:20:48,520 - INFO - Fetching data for GOOG\n",
      "2025-02-13 16:20:48,727 - INFO - Fetching 2025-02-14 options for GOOG\n",
      "2025-02-13 16:20:48,942 - INFO - Successfully fetched 2025-02-14 options for GOOG\n",
      "2025-02-13 16:20:48,944 - INFO - Fetching 2025-02-21 options for GOOG\n",
      "2025-02-13 16:20:49,237 - INFO - Successfully fetched 2025-02-21 options for GOOG\n",
      "2025-02-13 16:20:49,239 - INFO - Fetching 2025-02-28 options for GOOG\n",
      "2025-02-13 16:20:49,456 - INFO - Successfully fetched 2025-02-28 options for GOOG\n",
      "2025-02-13 16:20:49,459 - INFO - Fetching 2025-03-07 options for GOOG\n",
      "2025-02-13 16:20:49,663 - INFO - Successfully fetched 2025-03-07 options for GOOG\n",
      "2025-02-13 16:20:49,665 - INFO - Cached data for GOOG\n",
      "2025-02-13 16:20:49,897 - INFO - Using cached data for GOOG (age: 1.4s)\n",
      "2025-02-13 16:20:49,910 - INFO - Successfully analyzed GOOG in 1.39 seconds\n",
      "2025-02-13 16:20:49,915 - INFO - Using cached data for GOOG (age: 1.4s)\n",
      "2025-02-13 16:20:49,941 - INFO - \n",
      "Analyzing MO...\n",
      "2025-02-13 16:20:49,943 - INFO - Fetching data for MO\n",
      "2025-02-13 16:20:50,173 - INFO - Fetching 2025-02-14 options for MO\n",
      "2025-02-13 16:20:50,393 - INFO - Successfully fetched 2025-02-14 options for MO\n",
      "2025-02-13 16:20:50,398 - INFO - Fetching 2025-02-21 options for MO\n",
      "2025-02-13 16:20:50,605 - INFO - Successfully fetched 2025-02-21 options for MO\n",
      "2025-02-13 16:20:50,607 - INFO - Fetching 2025-02-28 options for MO\n",
      "2025-02-13 16:20:50,826 - INFO - Successfully fetched 2025-02-28 options for MO\n",
      "2025-02-13 16:20:50,834 - INFO - Fetching 2025-03-07 options for MO\n",
      "2025-02-13 16:20:51,041 - INFO - Successfully fetched 2025-03-07 options for MO\n",
      "2025-02-13 16:20:51,043 - INFO - Cached data for MO\n",
      "2025-02-13 16:20:51,352 - INFO - Using cached data for MO (age: 1.4s)\n",
      "2025-02-13 16:20:51,371 - INFO - Successfully analyzed MO in 1.43 seconds\n",
      "2025-02-13 16:20:51,373 - INFO - Using cached data for MO (age: 1.4s)\n",
      "2025-02-13 16:20:51,430 - INFO - \n",
      "Analyzing AMZN...\n",
      "2025-02-13 16:20:51,436 - INFO - Fetching data for AMZN\n",
      "2025-02-13 16:20:51,658 - INFO - Fetching 2025-02-14 options for AMZN\n",
      "2025-02-13 16:20:51,883 - INFO - Successfully fetched 2025-02-14 options for AMZN\n",
      "2025-02-13 16:20:51,886 - INFO - Fetching 2025-02-21 options for AMZN\n",
      "2025-02-13 16:20:52,160 - INFO - Successfully fetched 2025-02-21 options for AMZN\n",
      "2025-02-13 16:20:52,163 - INFO - Fetching 2025-02-28 options for AMZN\n",
      "2025-02-13 16:20:52,367 - INFO - Successfully fetched 2025-02-28 options for AMZN\n",
      "2025-02-13 16:20:52,369 - INFO - Fetching 2025-03-07 options for AMZN\n",
      "2025-02-13 16:20:52,604 - INFO - Successfully fetched 2025-03-07 options for AMZN\n",
      "2025-02-13 16:20:52,606 - INFO - Cached data for AMZN\n",
      "2025-02-13 16:20:52,876 - INFO - Using cached data for AMZN (age: 1.4s)\n",
      "2025-02-13 16:20:52,890 - INFO - Successfully analyzed AMZN in 1.45 seconds\n",
      "2025-02-13 16:20:52,892 - INFO - Using cached data for AMZN (age: 1.5s)\n",
      "2025-02-13 16:20:52,931 - INFO - \n",
      "Analyzing AMCR...\n",
      "2025-02-13 16:20:52,933 - INFO - Fetching data for AMCR\n",
      "2025-02-13 16:20:53,138 - INFO - Fetching 2025-02-21 options for AMCR\n",
      "2025-02-13 16:20:53,351 - INFO - Successfully fetched 2025-02-21 options for AMCR\n",
      "2025-02-13 16:20:53,353 - INFO - Fetching 2025-03-21 options for AMCR\n",
      "2025-02-13 16:20:53,546 - INFO - Successfully fetched 2025-03-21 options for AMCR\n",
      "2025-02-13 16:20:53,548 - INFO - Fetching 2025-04-17 options for AMCR\n",
      "2025-02-13 16:20:53,756 - INFO - Successfully fetched 2025-04-17 options for AMCR\n",
      "2025-02-13 16:20:53,758 - INFO - Fetching 2025-07-18 options for AMCR\n",
      "2025-02-13 16:20:54,026 - INFO - Successfully fetched 2025-07-18 options for AMCR\n",
      "2025-02-13 16:20:54,028 - INFO - Cached data for AMCR\n",
      "2025-02-13 16:20:54,271 - INFO - Using cached data for AMCR (age: 1.3s)\n",
      "2025-02-13 16:20:54,281 - INFO - Successfully analyzed AMCR in 1.35 seconds\n",
      "2025-02-13 16:20:54,282 - INFO - Using cached data for AMCR (age: 1.3s)\n",
      "2025-02-13 16:20:54,309 - INFO - \n",
      "Analyzing AEE...\n",
      "2025-02-13 16:20:54,314 - INFO - Fetching data for AEE\n",
      "2025-02-13 16:20:54,519 - INFO - Fetching 2025-02-21 options for AEE\n",
      "2025-02-13 16:20:54,723 - INFO - Successfully fetched 2025-02-21 options for AEE\n",
      "2025-02-13 16:20:54,724 - INFO - Fetching 2025-03-21 options for AEE\n",
      "2025-02-13 16:20:54,930 - INFO - Successfully fetched 2025-03-21 options for AEE\n",
      "2025-02-13 16:20:54,932 - INFO - Fetching 2025-06-20 options for AEE\n",
      "2025-02-13 16:20:55,171 - INFO - Successfully fetched 2025-06-20 options for AEE\n",
      "2025-02-13 16:20:55,173 - INFO - Fetching 2025-09-19 options for AEE\n",
      "2025-02-13 16:20:55,382 - INFO - Successfully fetched 2025-09-19 options for AEE\n",
      "2025-02-13 16:20:55,384 - INFO - Cached data for AEE\n",
      "2025-02-13 16:20:55,667 - INFO - Using cached data for AEE (age: 1.4s)\n",
      "2025-02-13 16:20:55,676 - INFO - Successfully analyzed AEE in 1.36 seconds\n",
      "2025-02-13 16:20:55,686 - INFO - Using cached data for AEE (age: 1.4s)\n",
      "2025-02-13 16:20:55,729 - INFO - \n",
      "Analyzing AEP...\n",
      "2025-02-13 16:20:55,732 - INFO - Fetching data for AEP\n",
      "2025-02-13 16:20:55,933 - INFO - Fetching 2025-02-21 options for AEP\n",
      "2025-02-13 16:20:56,245 - INFO - Successfully fetched 2025-02-21 options for AEP\n",
      "2025-02-13 16:20:56,247 - INFO - Fetching 2025-03-21 options for AEP\n",
      "2025-02-13 16:20:56,536 - INFO - Successfully fetched 2025-03-21 options for AEP\n",
      "2025-02-13 16:20:56,538 - INFO - Fetching 2025-05-16 options for AEP\n",
      "2025-02-13 16:20:56,742 - INFO - Successfully fetched 2025-05-16 options for AEP\n",
      "2025-02-13 16:20:56,744 - INFO - Fetching 2025-06-20 options for AEP\n",
      "2025-02-13 16:20:56,953 - INFO - Successfully fetched 2025-06-20 options for AEP\n",
      "2025-02-13 16:20:56,955 - INFO - Cached data for AEP\n",
      "2025-02-13 16:20:57,253 - INFO - Using cached data for AEP (age: 1.5s)\n",
      "2025-02-13 16:20:57,262 - INFO - Successfully analyzed AEP in 1.53 seconds\n",
      "2025-02-13 16:20:57,265 - INFO - Using cached data for AEP (age: 1.5s)\n",
      "2025-02-13 16:20:57,289 - INFO - \n",
      "Analyzing AXP...\n",
      "2025-02-13 16:20:57,292 - INFO - Fetching data for AXP\n",
      "2025-02-13 16:20:57,489 - INFO - Fetching 2025-02-14 options for AXP\n",
      "2025-02-13 16:20:57,695 - INFO - Successfully fetched 2025-02-14 options for AXP\n",
      "2025-02-13 16:20:57,699 - INFO - Fetching 2025-02-21 options for AXP\n",
      "2025-02-13 16:20:57,913 - INFO - Successfully fetched 2025-02-21 options for AXP\n",
      "2025-02-13 16:20:57,916 - INFO - Fetching 2025-02-28 options for AXP\n",
      "2025-02-13 16:20:58,135 - INFO - Successfully fetched 2025-02-28 options for AXP\n",
      "2025-02-13 16:20:58,138 - INFO - Fetching 2025-03-07 options for AXP\n",
      "2025-02-13 16:20:58,340 - INFO - Successfully fetched 2025-03-07 options for AXP\n",
      "2025-02-13 16:20:58,341 - INFO - Cached data for AXP\n",
      "2025-02-13 16:20:58,620 - INFO - Using cached data for AXP (age: 1.3s)\n",
      "2025-02-13 16:20:58,627 - INFO - Successfully analyzed AXP in 1.34 seconds\n",
      "2025-02-13 16:20:58,628 - INFO - Using cached data for AXP (age: 1.3s)\n",
      "2025-02-13 16:20:58,654 - INFO - \n",
      "Analyzing AIG...\n",
      "2025-02-13 16:20:58,656 - INFO - Fetching data for AIG\n",
      "2025-02-13 16:20:58,849 - INFO - Fetching 2025-02-14 options for AIG\n",
      "2025-02-13 16:20:59,047 - INFO - Successfully fetched 2025-02-14 options for AIG\n",
      "2025-02-13 16:20:59,049 - INFO - Fetching 2025-02-21 options for AIG\n",
      "2025-02-13 16:20:59,241 - INFO - Successfully fetched 2025-02-21 options for AIG\n",
      "2025-02-13 16:20:59,243 - INFO - Fetching 2025-02-28 options for AIG\n",
      "2025-02-13 16:20:59,450 - INFO - Successfully fetched 2025-02-28 options for AIG\n",
      "2025-02-13 16:20:59,455 - INFO - Fetching 2025-03-07 options for AIG\n",
      "2025-02-13 16:20:59,660 - INFO - Successfully fetched 2025-03-07 options for AIG\n",
      "2025-02-13 16:20:59,663 - INFO - Cached data for AIG\n",
      "2025-02-13 16:20:59,989 - INFO - Using cached data for AIG (age: 1.3s)\n",
      "2025-02-13 16:21:00,003 - INFO - Successfully analyzed AIG in 1.35 seconds\n",
      "2025-02-13 16:21:00,005 - INFO - Using cached data for AIG (age: 1.3s)\n",
      "2025-02-13 16:21:00,044 - INFO - \n",
      "Analyzing AMT...\n",
      "2025-02-13 16:21:00,053 - INFO - Fetching data for AMT\n",
      "2025-02-13 16:21:00,255 - INFO - Fetching 2025-02-21 options for AMT\n",
      "2025-02-13 16:21:00,462 - INFO - Successfully fetched 2025-02-21 options for AMT\n",
      "2025-02-13 16:21:00,465 - INFO - Fetching 2025-03-21 options for AMT\n",
      "2025-02-13 16:21:00,672 - INFO - Successfully fetched 2025-03-21 options for AMT\n",
      "2025-02-13 16:21:00,674 - INFO - Fetching 2025-04-17 options for AMT\n",
      "2025-02-13 16:21:00,876 - INFO - Successfully fetched 2025-04-17 options for AMT\n",
      "2025-02-13 16:21:00,880 - INFO - Fetching 2025-06-20 options for AMT\n",
      "2025-02-13 16:21:01,096 - INFO - Successfully fetched 2025-06-20 options for AMT\n",
      "2025-02-13 16:21:01,101 - INFO - Cached data for AMT\n",
      "2025-02-13 16:21:01,366 - INFO - Using cached data for AMT (age: 1.3s)\n",
      "2025-02-13 16:21:01,375 - INFO - Successfully analyzed AMT in 1.32 seconds\n",
      "2025-02-13 16:21:01,376 - INFO - Using cached data for AMT (age: 1.3s)\n",
      "2025-02-13 16:21:01,405 - INFO - \n",
      "Analyzing AWK...\n",
      "2025-02-13 16:21:01,408 - INFO - Fetching data for AWK\n",
      "2025-02-13 16:21:01,605 - INFO - Fetching 2025-02-21 options for AWK\n",
      "2025-02-13 16:21:01,817 - INFO - Successfully fetched 2025-02-21 options for AWK\n",
      "2025-02-13 16:21:01,820 - INFO - Fetching 2025-03-21 options for AWK\n",
      "2025-02-13 16:21:02,024 - INFO - Successfully fetched 2025-03-21 options for AWK\n",
      "2025-02-13 16:21:02,026 - INFO - Fetching 2025-06-20 options for AWK\n",
      "2025-02-13 16:21:02,237 - INFO - Successfully fetched 2025-06-20 options for AWK\n",
      "2025-02-13 16:21:02,239 - INFO - Fetching 2025-09-19 options for AWK\n",
      "2025-02-13 16:21:02,442 - INFO - Successfully fetched 2025-09-19 options for AWK\n",
      "2025-02-13 16:21:02,445 - INFO - Cached data for AWK\n",
      "2025-02-13 16:21:02,696 - INFO - Using cached data for AWK (age: 1.3s)\n",
      "2025-02-13 16:21:02,711 - INFO - Successfully analyzed AWK in 1.30 seconds\n",
      "2025-02-13 16:21:02,715 - INFO - Using cached data for AWK (age: 1.3s)\n",
      "2025-02-13 16:21:04,241 - INFO - \n",
      "Top 10 IV Crush Candidates:\n",
      "2025-02-13 16:21:04,243 - INFO - \n",
      "Symbol: AKAM\n",
      "2025-02-13 16:21:04,244 - INFO - IV Crush Probability: 100.00%\n",
      "2025-02-13 16:21:04,245 - INFO - Statistical Tests:\n",
      "2025-02-13 16:21:04,245 - INFO -   normality:\n",
      "2025-02-13 16:21:04,246 - INFO -     shapiro_p: 0.0000\n",
      "2025-02-13 16:21:04,247 - INFO -     ks_p: 0.0000\n",
      "2025-02-13 16:21:04,248 - INFO -     anderson_stat: 5.7540\n",
      "2025-02-13 16:21:04,249 - INFO -     anderson_critical: [0.5640, 0.6420, 0.7700, 0.8990, 1.0690]\n",
      "2025-02-13 16:21:04,249 - INFO -     jarque_bera_stat: 736.3684\n",
      "2025-02-13 16:21:04,249 - INFO -     jarque_bera_p: 0.0000\n",
      "2025-02-13 16:21:04,250 - INFO -   stationarity:\n",
      "2025-02-13 16:21:04,250 - INFO -     adf_stat: -4.4179\n",
      "2025-02-13 16:21:04,251 - INFO -     adf_p: 0.0003\n",
      "2025-02-13 16:21:04,251 - INFO -     kpss_stat: 0.2859\n",
      "2025-02-13 16:21:04,251 - INFO -     kpss_p: 0.1000\n",
      "2025-02-13 16:21:04,253 - INFO -   heteroscedasticity:\n",
      "2025-02-13 16:21:04,253 - INFO -     arch_lm_stat: 36.6262\n",
      "2025-02-13 16:21:04,254 - INFO -     arch_lm_p: 0.0001\n",
      "2025-02-13 16:21:04,254 - INFO -     arch_f_stat: 4.3663\n",
      "2025-02-13 16:21:04,254 - INFO -     arch_f_p: 0.0000\n",
      "2025-02-13 16:21:04,255 - INFO -   extremes:\n",
      "2025-02-13 16:21:04,256 - INFO -     extreme_count: 6.0000\n",
      "2025-02-13 16:21:04,257 - INFO -     extreme_ratio: 0.0333\n",
      "2025-02-13 16:21:04,257 - INFO -     var_95: 1.2067\n",
      "2025-02-13 16:21:04,258 - INFO -     var_99: 1.7981\n",
      "2025-02-13 16:21:04,258 - INFO -     es_95: 1.6448\n",
      "2025-02-13 16:21:04,258 - INFO -     es_99: 2.0639\n",
      "2025-02-13 16:21:04,259 - INFO -   autocorrelation:\n",
      "2025-02-13 16:21:04,259 - INFO -     lag1: 0.6901\n",
      "2025-02-13 16:21:04,260 - INFO -     lag5: 0.3385\n",
      "2025-02-13 16:21:04,261 - INFO - \n",
      "Symbol: ALGN\n",
      "2025-02-13 16:21:04,262 - INFO - IV Crush Probability: 100.00%\n",
      "2025-02-13 16:21:04,262 - INFO - Statistical Tests:\n",
      "2025-02-13 16:21:04,263 - INFO -   normality:\n",
      "2025-02-13 16:21:04,263 - INFO -     shapiro_p: 0.0000\n",
      "2025-02-13 16:21:04,264 - INFO -     ks_p: 0.0000\n",
      "2025-02-13 16:21:04,264 - INFO -     anderson_stat: 12.1806\n",
      "2025-02-13 16:21:04,264 - INFO -     anderson_critical: [0.5640, 0.6430, 0.7710, 0.8990, 1.0700]\n",
      "2025-02-13 16:21:04,265 - INFO -     jarque_bera_stat: 119.3594\n",
      "2025-02-13 16:21:04,265 - INFO -     jarque_bera_p: 0.0000\n",
      "2025-02-13 16:21:04,266 - INFO -   stationarity:\n",
      "2025-02-13 16:21:04,267 - INFO -     adf_stat: -3.7748\n",
      "2025-02-13 16:21:04,268 - INFO -     adf_p: 0.0032\n",
      "2025-02-13 16:21:04,270 - INFO -     kpss_stat: 0.5297\n",
      "2025-02-13 16:21:04,270 - INFO -     kpss_p: 0.0350\n",
      "2025-02-13 16:21:04,271 - INFO -   heteroscedasticity:\n",
      "2025-02-13 16:21:04,271 - INFO -     arch_lm_stat: 127.2347\n",
      "2025-02-13 16:21:04,272 - INFO -     arch_lm_p: 0.0000\n",
      "2025-02-13 16:21:04,272 - INFO -     arch_f_stat: 43.0505\n",
      "2025-02-13 16:21:04,273 - INFO -     arch_f_p: 0.0000\n",
      "2025-02-13 16:21:04,273 - INFO -   extremes:\n",
      "2025-02-13 16:21:04,273 - INFO -     extreme_count: 13.0000\n",
      "2025-02-13 16:21:04,274 - INFO -     extreme_ratio: 0.0699\n",
      "2025-02-13 16:21:04,274 - INFO -     var_95: 0.8442\n",
      "2025-02-13 16:21:04,274 - INFO -     var_99: 0.8648\n",
      "2025-02-13 16:21:04,275 - INFO -     es_95: 0.9016\n",
      "2025-02-13 16:21:04,275 - INFO -     es_99: 1.0985\n",
      "2025-02-13 16:21:04,276 - INFO -   autocorrelation:\n",
      "2025-02-13 16:21:04,276 - INFO -     lag1: 0.8691\n",
      "2025-02-13 16:21:04,276 - INFO -     lag5: 0.5029\n",
      "2025-02-13 16:21:04,277 - INFO - \n",
      "Symbol: AIG\n",
      "2025-02-13 16:21:04,277 - INFO - IV Crush Probability: 100.00%\n",
      "2025-02-13 16:21:04,278 - INFO - Statistical Tests:\n",
      "2025-02-13 16:21:04,278 - INFO -   normality:\n",
      "2025-02-13 16:21:04,280 - INFO -     shapiro_p: 0.0000\n",
      "2025-02-13 16:21:04,280 - INFO -     ks_p: 0.0000\n",
      "2025-02-13 16:21:04,281 - INFO -     anderson_stat: 9.7327\n",
      "2025-02-13 16:21:04,282 - INFO -     anderson_critical: [0.5600, 0.6370, 0.7650, 0.8920, 1.0610]\n",
      "2025-02-13 16:21:04,282 - INFO -     jarque_bera_stat: 66.0811\n",
      "2025-02-13 16:21:04,283 - INFO -     jarque_bera_p: 0.0000\n",
      "2025-02-13 16:21:04,283 - INFO -   stationarity:\n",
      "2025-02-13 16:21:04,284 - INFO -     adf_stat: -3.6428\n",
      "2025-02-13 16:21:04,284 - INFO -     adf_p: 0.0050\n",
      "2025-02-13 16:21:04,284 - INFO -     kpss_stat: 0.4366\n",
      "2025-02-13 16:21:04,285 - INFO -     kpss_p: 0.0614\n",
      "2025-02-13 16:21:04,285 - INFO -   heteroscedasticity:\n",
      "2025-02-13 16:21:04,286 - INFO -     arch_lm_stat: 57.6299\n",
      "2025-02-13 16:21:04,287 - INFO -     arch_lm_p: 0.0000\n",
      "2025-02-13 16:21:04,287 - INFO -     arch_f_stat: 10.0716\n",
      "2025-02-13 16:21:04,289 - INFO -     arch_f_p: 0.0000\n",
      "2025-02-13 16:21:04,290 - INFO -   extremes:\n",
      "2025-02-13 16:21:04,291 - INFO -     extreme_count: 9.0000\n",
      "2025-02-13 16:21:04,291 - INFO -     extreme_ratio: 0.0692\n",
      "2025-02-13 16:21:04,292 - INFO -     var_95: 0.7895\n",
      "2025-02-13 16:21:04,292 - INFO -     var_99: 1.0984\n",
      "2025-02-13 16:21:04,293 - INFO -     es_95: 0.9214\n",
      "2025-02-13 16:21:04,293 - INFO -     es_99: 1.2142\n",
      "2025-02-13 16:21:04,293 - INFO -   autocorrelation:\n",
      "2025-02-13 16:21:04,294 - INFO -     lag1: 0.7099\n",
      "2025-02-13 16:21:04,294 - INFO -     lag5: 0.2611\n",
      "2025-02-13 16:21:04,295 - INFO - \n",
      "Symbol: ABNB\n",
      "2025-02-13 16:21:04,296 - INFO - IV Crush Probability: 98.60%\n",
      "2025-02-13 16:21:04,298 - INFO - Statistical Tests:\n",
      "2025-02-13 16:21:04,299 - INFO -   normality:\n",
      "2025-02-13 16:21:04,299 - INFO -     shapiro_p: 0.0000\n",
      "2025-02-13 16:21:04,300 - INFO -     ks_p: 0.0000\n",
      "2025-02-13 16:21:04,300 - INFO -     anderson_stat: 18.8697\n",
      "2025-02-13 16:21:04,301 - INFO -     anderson_critical: [0.5690, 0.6480, 0.7770, 0.9070, 1.0790]\n",
      "2025-02-13 16:21:04,302 - INFO -     jarque_bera_stat: 224.2873\n",
      "2025-02-13 16:21:04,304 - INFO -     jarque_bera_p: 0.0000\n",
      "2025-02-13 16:21:04,305 - INFO -   stationarity:\n",
      "2025-02-13 16:21:04,305 - INFO -     adf_stat: -3.9075\n",
      "2025-02-13 16:21:04,306 - INFO -     adf_p: 0.0020\n",
      "2025-02-13 16:21:04,306 - INFO -     kpss_stat: 2.4776\n",
      "2025-02-13 16:21:04,307 - INFO -     kpss_p: 0.0100\n",
      "2025-02-13 16:21:04,307 - INFO -   heteroscedasticity:\n",
      "2025-02-13 16:21:04,308 - INFO -     arch_lm_stat: 215.3531\n",
      "2025-02-13 16:21:04,309 - INFO -     arch_lm_p: 0.0000\n",
      "2025-02-13 16:21:04,309 - INFO -     arch_f_stat: 71.1796\n",
      "2025-02-13 16:21:04,310 - INFO -     arch_f_p: 0.0000\n",
      "2025-02-13 16:21:04,310 - INFO -   extremes:\n",
      "2025-02-13 16:21:04,311 - INFO -     extreme_count: 11.0000\n",
      "2025-02-13 16:21:04,311 - INFO -     extreme_ratio: 0.0350\n",
      "2025-02-13 16:21:04,312 - INFO -     var_95: 0.9033\n",
      "2025-02-13 16:21:04,314 - INFO -     var_99: 0.9286\n",
      "2025-02-13 16:21:04,315 - INFO -     es_95: 1.0474\n",
      "2025-02-13 16:21:04,315 - INFO -     es_99: 1.4506\n",
      "2025-02-13 16:21:04,316 - INFO -   autocorrelation:\n",
      "2025-02-13 16:21:04,316 - INFO -     lag1: 0.8940\n",
      "2025-02-13 16:21:04,316 - INFO -     lag5: 0.7212\n",
      "2025-02-13 16:21:04,317 - INFO - \n",
      "Symbol: ACN\n",
      "2025-02-13 16:21:04,318 - INFO - IV Crush Probability: 98.40%\n",
      "2025-02-13 16:21:04,318 - INFO - Statistical Tests:\n",
      "2025-02-13 16:21:04,319 - INFO -   normality:\n",
      "2025-02-13 16:21:04,319 - INFO -     shapiro_p: 0.0000\n",
      "2025-02-13 16:21:04,320 - INFO -     ks_p: 0.0000\n",
      "2025-02-13 16:21:04,321 - INFO -     anderson_stat: 21.4530\n",
      "2025-02-13 16:21:04,321 - INFO -     anderson_critical: [0.5670, 0.6460, 0.7750, 0.9040, 1.0750]\n",
      "2025-02-13 16:21:04,322 - INFO -     jarque_bera_stat: 338.4112\n",
      "2025-02-13 16:21:04,323 - INFO -     jarque_bera_p: 0.0000\n",
      "2025-02-13 16:21:04,323 - INFO -   stationarity:\n",
      "2025-02-13 16:21:04,324 - INFO -     adf_stat: -3.4161\n",
      "2025-02-13 16:21:04,324 - INFO -     adf_p: 0.0104\n",
      "2025-02-13 16:21:04,325 - INFO -     kpss_stat: 0.3783\n",
      "2025-02-13 16:21:04,325 - INFO -     kpss_p: 0.0865\n",
      "2025-02-13 16:21:04,325 - INFO -   heteroscedasticity:\n",
      "2025-02-13 16:21:04,326 - INFO -     arch_lm_stat: 105.1130\n",
      "2025-02-13 16:21:04,326 - INFO -     arch_lm_p: 0.0000\n",
      "2025-02-13 16:21:04,326 - INFO -     arch_f_stat: 17.7913\n",
      "2025-02-13 16:21:04,327 - INFO -     arch_f_p: 0.0000\n",
      "2025-02-13 16:21:04,327 - INFO -   extremes:\n",
      "2025-02-13 16:21:04,327 - INFO -     extreme_count: 19.0000\n",
      "2025-02-13 16:21:04,328 - INFO -     extreme_ratio: 0.0757\n",
      "2025-02-13 16:21:04,328 - INFO -     var_95: 0.9455\n",
      "2025-02-13 16:21:04,329 - INFO -     var_99: 0.9455\n",
      "2025-02-13 16:21:04,330 - INFO -     es_95: 0.9455\n",
      "2025-02-13 16:21:04,331 - INFO -     es_99: 0.9455\n",
      "2025-02-13 16:21:04,331 - INFO -   autocorrelation:\n",
      "2025-02-13 16:21:04,332 - INFO -     lag1: 0.5779\n",
      "2025-02-13 16:21:04,333 - INFO -     lag5: 0.4394\n",
      "2025-02-13 16:21:04,334 - INFO - \n",
      "Symbol: AXP\n",
      "2025-02-13 16:21:04,334 - INFO - IV Crush Probability: 96.38%\n",
      "2025-02-13 16:21:04,335 - INFO - Statistical Tests:\n",
      "2025-02-13 16:21:04,335 - INFO -   normality:\n",
      "2025-02-13 16:21:04,336 - INFO -     shapiro_p: 0.0000\n",
      "2025-02-13 16:21:04,337 - INFO -     ks_p: 0.0000\n",
      "2025-02-13 16:21:04,338 - INFO -     anderson_stat: 22.7999\n",
      "2025-02-13 16:21:04,338 - INFO -     anderson_critical: [0.5660, 0.6450, 0.7740, 0.9020, 1.0730]\n",
      "2025-02-13 16:21:04,339 - INFO -     jarque_bera_stat: 694.7075\n",
      "2025-02-13 16:21:04,339 - INFO -     jarque_bera_p: 0.0000\n",
      "2025-02-13 16:21:04,340 - INFO -   stationarity:\n",
      "2025-02-13 16:21:04,340 - INFO -     adf_stat: -4.3407\n",
      "2025-02-13 16:21:04,341 - INFO -     adf_p: 0.0004\n",
      "2025-02-13 16:21:04,341 - INFO -     kpss_stat: 0.7969\n",
      "2025-02-13 16:21:04,342 - INFO -     kpss_p: 0.0100\n",
      "2025-02-13 16:21:04,343 - INFO -   heteroscedasticity:\n",
      "2025-02-13 16:21:04,343 - INFO -     arch_lm_stat: 100.5119\n",
      "2025-02-13 16:21:04,344 - INFO -     arch_lm_p: 0.0000\n",
      "2025-02-13 16:21:04,344 - INFO -     arch_f_stat: 17.9789\n",
      "2025-02-13 16:21:04,345 - INFO -     arch_f_p: 0.0000\n",
      "2025-02-13 16:21:04,346 - INFO -   extremes:\n",
      "2025-02-13 16:21:04,347 - INFO -     extreme_count: 13.0000\n",
      "2025-02-13 16:21:04,347 - INFO -     extreme_ratio: 0.0580\n",
      "2025-02-13 16:21:04,348 - INFO -     var_95: 0.8591\n",
      "2025-02-13 16:21:04,348 - INFO -     var_99: 0.8874\n",
      "2025-02-13 16:21:04,349 - INFO -     es_95: 0.8874\n",
      "2025-02-13 16:21:04,350 - INFO -     es_99: 0.8874\n",
      "2025-02-13 16:21:04,351 - INFO -   autocorrelation:\n",
      "2025-02-13 16:21:04,352 - INFO -     lag1: 0.7366\n",
      "2025-02-13 16:21:04,352 - INFO -     lag5: 0.4240\n",
      "2025-02-13 16:21:04,353 - INFO - \n",
      "Symbol: A\n",
      "2025-02-13 16:21:04,353 - INFO - IV Crush Probability: 91.50%\n",
      "2025-02-13 16:21:04,354 - INFO - Statistical Tests:\n",
      "2025-02-13 16:21:04,354 - INFO -   normality:\n",
      "2025-02-13 16:21:04,354 - INFO -     shapiro_p: 0.0000\n",
      "2025-02-13 16:21:04,355 - INFO -     ks_p: 0.0000\n",
      "2025-02-13 16:21:04,355 - INFO -     anderson_stat: 12.2854\n",
      "2025-02-13 16:21:04,355 - INFO -     anderson_critical: [0.5570, 0.6340, 0.7610, 0.8880, 1.0560]\n",
      "2025-02-13 16:21:04,356 - INFO -     jarque_bera_stat: 528.8496\n",
      "2025-02-13 16:21:04,357 - INFO -     jarque_bera_p: 0.0000\n",
      "2025-02-13 16:21:04,357 - INFO -   stationarity:\n",
      "2025-02-13 16:21:04,357 - INFO -     adf_stat: -4.1267\n",
      "2025-02-13 16:21:04,357 - INFO -     adf_p: 0.0009\n",
      "2025-02-13 16:21:04,358 - INFO -     kpss_stat: 0.7840\n",
      "2025-02-13 16:21:04,358 - INFO -     kpss_p: 0.0100\n",
      "2025-02-13 16:21:04,358 - INFO -   heteroscedasticity:\n",
      "2025-02-13 16:21:04,359 - INFO -     arch_lm_stat: 62.5226\n",
      "2025-02-13 16:21:04,359 - INFO -     arch_lm_p: 0.0000\n",
      "2025-02-13 16:21:04,360 - INFO -     arch_f_stat: 14.6242\n",
      "2025-02-13 16:21:04,360 - INFO -     arch_f_p: 0.0000\n",
      "2025-02-13 16:21:04,360 - INFO -   extremes:\n",
      "2025-02-13 16:21:04,361 - INFO -     extreme_count: 7.0000\n",
      "2025-02-13 16:21:04,361 - INFO -     extreme_ratio: 0.0631\n",
      "2025-02-13 16:21:04,362 - INFO -     var_95: 0.6278\n",
      "2025-02-13 16:21:04,363 - INFO -     var_99: 1.0864\n",
      "2025-02-13 16:21:04,363 - INFO -     es_95: 0.8441\n",
      "2025-02-13 16:21:04,364 - INFO -     es_99: 1.1107\n",
      "2025-02-13 16:21:04,364 - INFO -   autocorrelation:\n",
      "2025-02-13 16:21:04,365 - INFO -     lag1: 0.7644\n",
      "2025-02-13 16:21:04,366 - INFO -     lag5: 0.1246\n",
      "2025-02-13 16:21:04,367 - INFO - \n",
      "Symbol: AES\n",
      "2025-02-13 16:21:04,368 - INFO - IV Crush Probability: 87.60%\n",
      "2025-02-13 16:21:04,368 - INFO - Statistical Tests:\n",
      "2025-02-13 16:21:04,369 - INFO -   normality:\n",
      "2025-02-13 16:21:04,369 - INFO -     shapiro_p: 0.0000\n",
      "2025-02-13 16:21:04,370 - INFO -     ks_p: 0.0000\n",
      "2025-02-13 16:21:04,370 - INFO -     anderson_stat: 13.5948\n",
      "2025-02-13 16:21:04,371 - INFO -     anderson_critical: [0.5590, 0.6360, 0.7630, 0.8910, 1.0590]\n",
      "2025-02-13 16:21:04,371 - INFO -     jarque_bera_stat: 2354.0965\n",
      "2025-02-13 16:21:04,371 - INFO -     jarque_bera_p: 0.0000\n",
      "2025-02-13 16:21:04,372 - INFO -   stationarity:\n",
      "2025-02-13 16:21:04,372 - INFO -     adf_stat: -4.1269\n",
      "2025-02-13 16:21:04,373 - INFO -     adf_p: 0.0009\n",
      "2025-02-13 16:21:04,374 - INFO -     kpss_stat: 0.4271\n",
      "2025-02-13 16:21:04,375 - INFO -     kpss_p: 0.0655\n",
      "2025-02-13 16:21:04,376 - INFO -   heteroscedasticity:\n",
      "2025-02-13 16:21:04,377 - INFO -     arch_lm_stat: 35.5624\n",
      "2025-02-13 16:21:04,377 - INFO -     arch_lm_p: 0.0001\n",
      "2025-02-13 16:21:04,378 - INFO -     arch_f_stat: 4.6842\n",
      "2025-02-13 16:21:04,380 - INFO -     arch_f_p: 0.0000\n",
      "2025-02-13 16:21:04,382 - INFO -   extremes:\n",
      "2025-02-13 16:21:04,383 - INFO -     extreme_count: 5.0000\n",
      "2025-02-13 16:21:04,384 - INFO -     extreme_ratio: 0.0407\n",
      "2025-02-13 16:21:04,385 - INFO -     var_95: 0.7603\n",
      "2025-02-13 16:21:04,385 - INFO -     var_99: 0.7603\n",
      "2025-02-13 16:21:04,386 - INFO -     es_95: 0.7603\n",
      "2025-02-13 16:21:04,386 - INFO -     es_99: 0.7603\n",
      "2025-02-13 16:21:04,387 - INFO -   autocorrelation:\n",
      "2025-02-13 16:21:04,389 - INFO -     lag1: 0.7756\n",
      "2025-02-13 16:21:04,390 - INFO -     lag5: 0.2263\n",
      "2025-02-13 16:21:04,390 - INFO - \n",
      "Symbol: ALLE\n",
      "2025-02-13 16:21:04,391 - INFO - IV Crush Probability: 83.30%\n",
      "2025-02-13 16:21:04,392 - INFO - Statistical Tests:\n",
      "2025-02-13 16:21:04,392 - INFO -   normality:\n",
      "2025-02-13 16:21:04,392 - INFO -     shapiro_p: 0.0002\n",
      "2025-02-13 16:21:04,393 - INFO -     ks_p: 0.0000\n",
      "2025-02-13 16:21:04,393 - INFO -     anderson_stat: 1.6700\n",
      "2025-02-13 16:21:04,394 - INFO -     anderson_critical: [0.5440, 0.6200, 0.7440, 0.8680, 1.0320]\n",
      "2025-02-13 16:21:04,394 - INFO -     jarque_bera_stat: 40.3218\n",
      "2025-02-13 16:21:04,394 - INFO -     jarque_bera_p: 0.0000\n",
      "2025-02-13 16:21:04,395 - INFO -   stationarity:\n",
      "2025-02-13 16:21:04,396 - INFO -     adf_stat: -4.4344\n",
      "2025-02-13 16:21:04,397 - INFO -     adf_p: 0.0003\n",
      "2025-02-13 16:21:04,398 - INFO -     kpss_stat: 0.8180\n",
      "2025-02-13 16:21:04,399 - INFO -     kpss_p: 0.0100\n",
      "2025-02-13 16:21:04,400 - INFO -   heteroscedasticity:\n",
      "2025-02-13 16:21:04,401 - INFO -     arch_lm_stat: 12.8387\n",
      "2025-02-13 16:21:04,401 - INFO -     arch_lm_p: 0.2328\n",
      "2025-02-13 16:21:04,401 - INFO -     arch_f_stat: 1.3441\n",
      "2025-02-13 16:21:04,402 - INFO -     arch_f_p: 0.2405\n",
      "2025-02-13 16:21:04,402 - INFO -   extremes:\n",
      "2025-02-13 16:21:04,403 - INFO -     extreme_count: 5.0000\n",
      "2025-02-13 16:21:04,404 - INFO -     extreme_ratio: 0.0806\n",
      "2025-02-13 16:21:04,405 - INFO -     var_95: 0.9848\n",
      "2025-02-13 16:21:04,405 - INFO -     var_99: 2.1358\n",
      "2025-02-13 16:21:04,405 - INFO -     es_95: 1.7766\n",
      "2025-02-13 16:21:04,406 - INFO -     es_99: 2.1358\n",
      "2025-02-13 16:21:04,406 - INFO -   autocorrelation:\n",
      "2025-02-13 16:21:04,406 - INFO -     lag1: 0.5063\n",
      "2025-02-13 16:21:04,407 - INFO -     lag5: 0.2817\n",
      "2025-02-13 16:21:04,408 - INFO - \n",
      "Symbol: AOS\n",
      "2025-02-13 16:21:04,409 - INFO - IV Crush Probability: 82.25%\n",
      "2025-02-13 16:21:04,411 - INFO - Statistical Tests:\n",
      "2025-02-13 16:21:04,411 - INFO -   normality:\n",
      "2025-02-13 16:21:04,412 - INFO -     shapiro_p: 0.0001\n",
      "2025-02-13 16:21:04,413 - INFO -     ks_p: 0.0000\n",
      "2025-02-13 16:21:04,414 - INFO -     anderson_stat: 2.2154\n",
      "2025-02-13 16:21:04,414 - INFO -     anderson_critical: [0.5500, 0.6260, 0.7510, 0.8760, 1.0420]\n",
      "2025-02-13 16:21:04,415 - INFO -     jarque_bera_stat: 14.4632\n",
      "2025-02-13 16:21:04,415 - INFO -     jarque_bera_p: 0.0007\n",
      "2025-02-13 16:21:04,416 - INFO -   stationarity:\n",
      "2025-02-13 16:21:04,416 - INFO -     adf_stat: -4.9172\n",
      "2025-02-13 16:21:04,417 - INFO -     adf_p: 0.0000\n",
      "2025-02-13 16:21:04,417 - INFO -     kpss_stat: 0.5947\n",
      "2025-02-13 16:21:04,418 - INFO -     kpss_p: 0.0231\n",
      "2025-02-13 16:21:04,418 - INFO -   heteroscedasticity:\n",
      "2025-02-13 16:21:04,419 - INFO -     arch_lm_stat: 17.5218\n",
      "2025-02-13 16:21:04,419 - INFO -     arch_lm_p: 0.0636\n",
      "2025-02-13 16:21:04,420 - INFO -     arch_f_stat: 1.9831\n",
      "2025-02-13 16:21:04,420 - INFO -     arch_f_p: 0.0527\n",
      "2025-02-13 16:21:04,421 - INFO -   extremes:\n",
      "2025-02-13 16:21:04,422 - INFO -     extreme_count: 3.0000\n",
      "2025-02-13 16:21:04,422 - INFO -     extreme_ratio: 0.0390\n",
      "2025-02-13 16:21:04,423 - INFO -     var_95: 1.5760\n",
      "2025-02-13 16:21:04,423 - INFO -     var_99: 1.5760\n",
      "2025-02-13 16:21:04,424 - INFO -     es_95: 1.5760\n",
      "2025-02-13 16:21:04,424 - INFO -     es_99: 1.5760\n",
      "2025-02-13 16:21:04,425 - INFO -   autocorrelation:\n",
      "2025-02-13 16:21:04,425 - INFO -     lag1: 0.4189\n",
      "2025-02-13 16:21:04,426 - INFO -     lag5: -0.0938\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import yfinance as yf\n",
    "import scipy.stats as stats\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm, ks_2samp, anderson, jarque_bera\n",
    "from statsmodels.stats.diagnostic import het_arch\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class IVCrushAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.sp500_symbols = self._get_sp500_symbols()\n",
    "        self.iv_data = {}\n",
    "        self.hv_data = {}\n",
    "        self.options_data = {}\n",
    "        self.cache = {}  # Add cache for API responses\n",
    "        self.cache_timestamp = {}  # Add timestamps for cache entries\n",
    "        logger.info(f\"Initialized analyzer with {len(self.sp500_symbols)} symbols\")\n",
    "\n",
    "    def _get_sp500_symbols(self) -> List[str]:\n",
    "        \"\"\"Fetch S&P 500 symbols using yfinance\"\"\"\n",
    "        try:\n",
    "            sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "            return sp500['Symbol'].tolist()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching S&P 500 symbols: {e}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_options_data(self, symbol: str, timeout: int = 5, cache_expiry: int = 300) -> Dict:\n",
    "        \"\"\"Fetch options chain data for a symbol with timeout and caching\"\"\"\n",
    "        cache_key = f\"{symbol}_options\"\n",
    "        current_time = time.time()\n",
    "\n",
    "        # Check cache with expiry\n",
    "        if cache_key in self.cache:\n",
    "            cache_age = current_time - self.cache_timestamp.get(cache_key, 0)\n",
    "            if cache_age < cache_expiry:\n",
    "                logger.info(f\"Using cached data for {symbol} (age: {cache_age:.1f}s)\")\n",
    "                return self.cache[cache_key]\n",
    "            else:\n",
    "                logger.info(f\"Cache expired for {symbol}\")\n",
    "\n",
    "        try:\n",
    "            start_time = current_time\n",
    "            logger.info(f\"Fetching data for {symbol}\")\n",
    "            stock = yf.Ticker(symbol)\n",
    "            options_data = {}\n",
    "\n",
    "            for expiry in stock.options[:4]:  # Get first 4 expiration dates\n",
    "                if time.time() - start_time > timeout:\n",
    "                    logger.warning(f\"Timeout fetching options data for {symbol}\")\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    logger.info(f\"Fetching {expiry} options for {symbol}\")\n",
    "                    opt = stock.option_chain(expiry)\n",
    "                    if opt.calls.empty or opt.puts.empty:\n",
    "                        logger.warning(f\"Empty options data for {symbol} expiry {expiry}\")\n",
    "                        continue\n",
    "\n",
    "                    options_data[expiry] = {\n",
    "                        'calls': opt.calls,\n",
    "                        'puts': opt.puts,\n",
    "                        'expiry': expiry\n",
    "                    }\n",
    "                    logger.info(f\"Successfully fetched {expiry} options for {symbol}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error fetching options data for {symbol} expiry {expiry}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            if options_data:\n",
    "                self.cache[cache_key] = options_data\n",
    "                self.cache_timestamp[cache_key] = current_time\n",
    "                logger.info(f\"Cached data for {symbol}\")\n",
    "            return options_data\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in fetch_options_data for {symbol}: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def calculate_iv_crush_probability(self, symbol: str, timeout: int = 15) -> Dict:\n",
    "        \"\"\"Calculate probability of IV crush based on all indicators\"\"\"\n",
    "        logger.info(f\"\\nAnalyzing {symbol}...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Fetch options data with timeout\n",
    "            if time.time() - start_time > timeout:\n",
    "                logger.warning(f\"Timeout during analysis of {symbol}\")\n",
    "                return {}\n",
    "\n",
    "            options_data = self.fetch_options_data(symbol)\n",
    "            if not options_data:\n",
    "                logger.warning(f\"No options data available for {symbol}\")\n",
    "                return {}\n",
    "\n",
    "            # Quick validation of required data\n",
    "            if len(options_data) < 2:\n",
    "                logger.warning(f\"Insufficient options data for {symbol}\")\n",
    "                return {}\n",
    "\n",
    "            iv_term = self.calculate_term_structure(options_data)\n",
    "            if len(iv_term) < 2:\n",
    "                logger.warning(f\"Insufficient term structure data for {symbol}\")\n",
    "                return {}\n",
    "\n",
    "            front_back_ratio = iv_term[min(iv_term.keys())] / iv_term[max(iv_term.keys())]\n",
    "\n",
    "            # Check timeout before heavy computations\n",
    "            if time.time() - start_time > timeout:\n",
    "                logger.warning(f\"Timeout during analysis of {symbol}\")\n",
    "                return {}\n",
    "\n",
    "            iv_hv_data = self.analyze_iv_hv_spread(symbol)\n",
    "            volume_data = self.analyze_volume_signals(options_data)\n",
    "            spread_data = self.analyze_bid_ask_spreads(options_data)\n",
    "\n",
    "            detail_scores = {\n",
    "                'term_structure_score': self._score_term_structure(front_back_ratio),\n",
    "                'iv_hv_score': self._score_iv_hv_spread(iv_hv_data.get('iv_hv_ratio', 0)),\n",
    "                'volume_score': self._score_volume_patterns(volume_data),\n",
    "                'spread_score': self._score_bid_ask_patterns(spread_data)\n",
    "            }\n",
    "\n",
    "            weights = {\n",
    "                'term_structure_score': 0.35,\n",
    "                'iv_hv_score': 0.30,\n",
    "                'volume_score': 0.20,\n",
    "                'spread_score': 0.15\n",
    "            }\n",
    "\n",
    "            total_probability = sum(detail_scores[k] * weights[k] for k in detail_scores.keys())\n",
    "\n",
    "            result = {\n",
    "                'symbol': symbol,\n",
    "                'iv_crush_probability': total_probability,\n",
    "                'detail_scores': detail_scores,\n",
    "                'raw_metrics': {\n",
    "                    'front_back_ratio': front_back_ratio,\n",
    "                    'iv_hv_ratio': iv_hv_data.get('iv_hv_ratio', 0),\n",
    "                    'volume_ratio': volume_data.get('volume_ratio', 0),\n",
    "                    'spread_ratio': max(spread_data.values()) / min(spread_data.values()) if spread_data else 0\n",
    "                }\n",
    "            }\n",
    "            logger.info(f\"Successfully analyzed {symbol} in {time.time() - start_time:.2f} seconds\")\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating IV crush probability for {symbol}: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def calculate_term_structure(self, options_data: Dict) -> Dict[str, float]:\n",
    "        \"\"\"Calculate IV term structure metrics\"\"\"\n",
    "        if not options_data:\n",
    "            logger.warning(\"No options data available for term structure calculation\")\n",
    "            return {}\n",
    "\n",
    "        expiries = sorted(options_data.keys())\n",
    "        if len(expiries) < 2:\n",
    "            logger.warning(\"Insufficient expiries for term structure calculation\")\n",
    "            return {}\n",
    "\n",
    "        iv_term = {}\n",
    "        for expiry in expiries:\n",
    "            try:\n",
    "                calls = options_data[expiry]['calls']\n",
    "                puts = options_data[expiry]['puts']\n",
    "\n",
    "                weight = (calls['volume'] * calls['openInterest']).fillna(0)\n",
    "                weighted_iv = (calls['impliedVolatility'] * weight).sum() / weight.sum() if weight.sum() > 0 else 0\n",
    "\n",
    "                iv_term[expiry] = weighted_iv\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error calculating term structure for expiry {expiry}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return iv_term\n",
    "\n",
    "    def analyze_iv_hv_spread(self, symbol: str) -> Dict[str, float]:\n",
    "        \"\"\"Analyze spread between IV and historical volatility\"\"\"\n",
    "        try:\n",
    "            stock = yf.Ticker(symbol)\n",
    "\n",
    "            hist_data = stock.history(period='1y')\n",
    "            if len(hist_data) < 20:\n",
    "                return {}\n",
    "\n",
    "            returns = np.log(hist_data['Close'] / hist_data['Close'].shift(1))\n",
    "            hv_20 = returns.rolling(window=20).std() * np.sqrt(252)\n",
    "            hv_current = hv_20.iloc[-1]\n",
    "\n",
    "            options = self.fetch_options_data(symbol)\n",
    "            if not options:\n",
    "                return {}\n",
    "\n",
    "            front_month_iv = self.calculate_term_structure(options).get(min(options.keys()))\n",
    "            if not front_month_iv:\n",
    "                return {}\n",
    "\n",
    "            return {\n",
    "                'hv': hv_current,\n",
    "                'iv': front_month_iv,\n",
    "                'iv_hv_ratio': front_month_iv / hv_current if hv_current > 0 else 0\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing IV-HV spread for {symbol}: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def analyze_volume_signals(self, options_data: Dict) -> Dict[str, float]:\n",
    "        \"\"\"Analyze options volume and open interest patterns\"\"\"\n",
    "        if not options_data:\n",
    "            return {'volume_ratio': 0, 'oi_ratio': 0}\n",
    "\n",
    "        try:\n",
    "            near_term = min(options_data.keys())\n",
    "            far_term = max(options_data.keys())\n",
    "\n",
    "            near_calls = options_data[near_term]['calls']\n",
    "            near_puts = options_data[near_term]['puts']\n",
    "            far_calls = options_data[far_term]['calls']\n",
    "            far_puts = options_data[far_term]['puts']\n",
    "\n",
    "            near_volume = (near_calls['volume'].sum() + near_puts['volume'].sum())\n",
    "            far_volume = (far_calls['volume'].sum() + far_puts['volume'].sum())\n",
    "            volume_ratio = near_volume / far_volume if far_volume > 0 else 0\n",
    "\n",
    "            near_oi = (near_calls['openInterest'].sum() + near_puts['openInterest'].sum())\n",
    "            far_oi = (far_calls['openInterest'].sum() + far_puts['openInterest'].sum())\n",
    "            oi_ratio = near_oi / far_oi if far_oi > 0 else 0\n",
    "\n",
    "            return {\n",
    "                'volume_ratio': volume_ratio,\n",
    "                'oi_ratio': oi_ratio\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing volume signals: {e}\")\n",
    "            return {'volume_ratio': 0, 'oi_ratio': 0}\n",
    "\n",
    "\n",
    "    def analyze_bid_ask_spreads(self, options_data: Dict) -> Dict[str, float]:\n",
    "        \"\"\"Analyze bid-ask spread patterns\"\"\"\n",
    "        if not options_data:\n",
    "            return {}\n",
    "\n",
    "        spreads = {}\n",
    "        for expiry, data in options_data.items():\n",
    "            calls = data['calls']\n",
    "            puts = data['puts']\n",
    "\n",
    "            call_spreads = (calls['ask'] - calls['bid']) / ((calls['ask'] + calls['bid']) / 2)\n",
    "            put_spreads = (puts['ask'] - puts['bid']) / ((puts['ask'] + puts['bid']) / 2)\n",
    "\n",
    "            spreads[expiry] = (call_spreads.mean() + put_spreads.mean()) / 2\n",
    "\n",
    "        return spreads\n",
    "\n",
    "    def _score_term_structure(self, ratio: float) -> float:\n",
    "        \"\"\"Score term structure steepness\"\"\"\n",
    "        if ratio > 1.3:\n",
    "            return 0.8\n",
    "        elif ratio > 1.2:\n",
    "            return 0.6\n",
    "        elif ratio > 1.1:\n",
    "            return 0.4\n",
    "        return 0.2\n",
    "\n",
    "    def _score_iv_hv_spread(self, ratio: float) -> float:\n",
    "        \"\"\"Score IV-HV spread\"\"\"\n",
    "        if ratio > 2.0:\n",
    "            return 0.9\n",
    "        elif ratio > 1.5:\n",
    "            return 0.7\n",
    "        elif ratio > 1.2:\n",
    "            return 0.5\n",
    "        return 0.3\n",
    "\n",
    "    def _score_volume_patterns(self, volume_data: Dict) -> float:\n",
    "        \"\"\"Score volume and OI patterns\"\"\"\n",
    "        volume_ratio = volume_data.get('volume_ratio', 0)\n",
    "        oi_ratio = volume_data.get('oi_ratio', 0)\n",
    "\n",
    "        if volume_ratio > 3.0 and oi_ratio > 2.0:\n",
    "            return 0.9\n",
    "        elif volume_ratio > 2.0 and oi_ratio > 1.5:\n",
    "            return 0.7\n",
    "        elif volume_ratio > 1.5 and oi_ratio > 1.2:\n",
    "            return 0.5\n",
    "        return 0.3\n",
    "\n",
    "    def _score_bid_ask_patterns(self, spread_data: Dict) -> float:\n",
    "        \"\"\"Score bid-ask spread patterns\"\"\"\n",
    "        if not spread_data:\n",
    "            return 0\n",
    "\n",
    "        spread_ratio = max(spread_data.values()) / min(spread_data.values())\n",
    "\n",
    "        if spread_ratio > 2.0:\n",
    "            return 0.8\n",
    "        elif spread_ratio > 1.5:\n",
    "            return 0.6\n",
    "        elif spread_ratio > 1.2:\n",
    "            return 0.4\n",
    "        return 0.2\n",
    "\n",
    "    def visualize_results(self, results: List[Dict]):\n",
    "        \"\"\"Visualize analysis results\"\"\"\n",
    "        if not results:\n",
    "            logger.warning(\"No results to visualize\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"\\nVisualizing results for {len(results)} symbols...\")\n",
    "\n",
    "        try:\n",
    "            # Convert results to DataFrame\n",
    "            df = pd.DataFrame(results)\n",
    "\n",
    "            # First plot: Probability Distribution\n",
    "            logger.info(\"Creating probability distribution plot...\")\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            if 'iv_crush_probability' in df.columns:\n",
    "                sns.kdeplot(data=df, x='iv_crush_probability', fill=True)\n",
    "                plt.title('IV Crush Probability Distribution')\n",
    "                plt.xlabel('Probability')\n",
    "                plt.ylabel('Density')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.savefig('probability_distribution.png')\n",
    "                plt.close()\n",
    "            else:\n",
    "                logger.warning(\"No probability data available for visualization\")\n",
    "\n",
    "            # Second plot: Component Scores\n",
    "            logger.info(\"Creating component scores plot...\")\n",
    "            plt.figure(figsize=(15, 8))\n",
    "\n",
    "            # Extract scores safely\n",
    "            scores_data = []\n",
    "            for idx, row in df.iterrows():\n",
    "                if isinstance(row.get('detail_scores'), dict):\n",
    "                    score_dict = {\n",
    "                        'symbol': row['symbol'],\n",
    "                    }\n",
    "                    # Safely extract each score component\n",
    "                    if isinstance(row.get('detail_scores'), dict):\n",
    "                        for key, value in row['detail_scores'].items():\n",
    "                            score_dict[key] = value\n",
    "                    scores_data.append(score_dict)\n",
    "\n",
    "            if scores_data:\n",
    "                scores_df = pd.DataFrame(scores_data)\n",
    "                if not scores_df.empty and len(scores_df.columns) > 1:  # Ensure we have more than just the symbol column\n",
    "                    scores_df.set_index('symbol', inplace=True)\n",
    "\n",
    "                    # Create heatmap\n",
    "                    sns.heatmap(scores_df, annot=True, cmap='YlOrRd', center=0.5)\n",
    "                    plt.title('Component Scores Comparison')\n",
    "                    plt.xlabel('Score Components')\n",
    "                    plt.ylabel('Symbols')\n",
    "                    plt.xticks(rotation=45)\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig('component_scores.png')\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    logger.warning(\"Insufficient data for component scores visualization\")\n",
    "            else:\n",
    "                logger.warning(\"No score data available for visualization\")\n",
    "\n",
    "            # Display text results\n",
    "            logger.info(\"\\nAnalysis Results Summary:\")\n",
    "            summary_df = df[['symbol', 'iv_crush_probability']].sort_values('iv_crush_probability', ascending=False)\n",
    "            logger.info(\"\\nTop IV Crush Candidates:\")\n",
    "            logger.info(\"\\n\" + str(summary_df))\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in visualization: {e}\")\n",
    "            logger.exception(\"Full visualization error traceback:\")\n",
    "\n",
    "\n",
    "\n",
    "class IVCrushAnalyzerEnhanced(IVCrushAnalyzer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.historical_iv = {}\n",
    "        self.statistical_tests = {}\n",
    "        self.profiling_enabled = False\n",
    "        self.profile = None\n",
    "\n",
    "    def enable_profiling(self):\n",
    "        \"\"\"Enable performance profiling\"\"\"\n",
    "        self.profiling_enabled = True\n",
    "        self.profile = cProfile.Profile()\n",
    "\n",
    "    def disable_profiling(self):\n",
    "        \"\"\"Disable performance profiling and print stats\"\"\"\n",
    "        if self.profile:\n",
    "            self.profile.disable()\n",
    "            stats = pstats.Stats(self.profile)\n",
    "            stats.sort_stats('cumulative')\n",
    "            stats.print_stats()\n",
    "        self.profiling_enabled = False\n",
    "\n",
    "    def _calculate_evt_metrics(self, series: np.array) -> Dict[str, float]:\n",
    "        \"\"\"Calculate EVT metrics with enhanced error handling and normalization\"\"\"\n",
    "        try:\n",
    "            # Handle edge cases\n",
    "            if len(series) < 2 or np.all(np.isnan(series)):\n",
    "                logger.warning(\"Insufficient data for EVT calculation\")\n",
    "                return {\n",
    "                    'var_95': 0,\n",
    "                    'var_99': 0,\n",
    "                    'es_95': 0,\n",
    "                    'es_99': 0\n",
    "                }\n",
    "\n",
    "            # Remove NaN values\n",
    "            series = series[~np.isnan(series)]\n",
    "\n",
    "            # Normalize returns for consistent scaling\n",
    "            normalized_series = (series - np.mean(series)) / np.std(series)\n",
    "\n",
    "            # Calculate VaR with error handling\n",
    "            var_95 = np.percentile(normalized_series, 5)\n",
    "            var_99 = np.percentile(normalized_series, 1)\n",
    "\n",
    "            # Calculate ES with error handling\n",
    "            es_95 = np.mean(normalized_series[normalized_series <= var_95]) if var_95 != 0 else 0\n",
    "            es_99 = np.mean(normalized_series[normalized_series <= var_99]) if var_99 != 0 else 0\n",
    "\n",
    "            # Handle potential negative values\n",
    "            return {\n",
    "                'var_95': abs(var_95),\n",
    "                'var_99': abs(var_99),\n",
    "                'es_95': abs(es_95),\n",
    "                'es_99': abs(es_99)\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating EVT metrics: {e}\")\n",
    "            return {\n",
    "                'var_95': 0,\n",
    "                'var_99': 0,\n",
    "                'es_95': 0,\n",
    "                'es_99': 0\n",
    "            }\n",
    "\n",
    "    def run_statistical_tests(self, symbol: str, options_data: Dict) -> Dict:\n",
    "        \"\"\"Run comprehensive statistical tests with enhanced error handling\"\"\"\n",
    "        if self.profiling_enabled:\n",
    "            self.profile.enable()\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            iv_series = self._extract_iv_series(options_data)\n",
    "            if len(iv_series) < 2:\n",
    "                logger.warning(f\"Insufficient data for statistical tests on {symbol}\")\n",
    "                return {}\n",
    "\n",
    "            # Remove NaN values and ensure sufficient data\n",
    "            iv_series = iv_series[~np.isnan(iv_series)]\n",
    "            if len(iv_series) < 2:\n",
    "                logger.warning(f\"Insufficient valid data points after NaN removal for {symbol}\")\n",
    "                return {}\n",
    "\n",
    "            # Extended Normality Tests with error handling\n",
    "            try:\n",
    "                _, shapiro_p = stats.shapiro(iv_series)\n",
    "                _, ks_p = stats.kstest(iv_series, 'norm')\n",
    "                stat, crit, sig = anderson(iv_series)\n",
    "                jb_stat, jb_p = jarque_bera(iv_series)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in normality tests for {symbol}: {e}\")\n",
    "                shapiro_p = ks_p = jb_p = 1.0\n",
    "                stat = crit = 0.0\n",
    "                jb_stat = 0.0\n",
    "\n",
    "            # Extended Stationarity Tests with error handling\n",
    "            try:\n",
    "                adf_stat, adf_p, _ = self._adf_test(iv_series)\n",
    "                # Handle KPSS test warnings gracefully\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    kpss_stat, kpss_p, _, _ = kpss(iv_series, regression='c', nlags='auto')\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in stationarity tests for {symbol}: {e}\")\n",
    "                adf_stat = kpss_stat = 0.0\n",
    "                adf_p = kpss_p = 1.0\n",
    "\n",
    "            # Enhanced Extreme Value Analysis\n",
    "            z_scores = stats.zscore(iv_series)\n",
    "            extremes = np.abs(z_scores) > 2\n",
    "            evt_metrics = self._calculate_evt_metrics(iv_series)\n",
    "\n",
    "            # Heteroscedasticity Test with error handling\n",
    "            try:\n",
    "                lm_stat, lm_p, f_stat, f_p = het_arch(iv_series)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in heteroscedasticity test for {symbol}: {e}\")\n",
    "                lm_stat = f_stat = 0.0\n",
    "                lm_p = f_p = 1.0\n",
    "\n",
    "            # Autocorrelation Test with improved handling\n",
    "            acf = self._autocorrelation(iv_series)\n",
    "\n",
    "            result = {\n",
    "                'normality': {\n",
    "                    'shapiro_p': shapiro_p,\n",
    "                    'ks_p': ks_p,\n",
    "                    'anderson_stat': stat,\n",
    "                    'anderson_critical': crit,\n",
    "                    'jarque_bera_stat': jb_stat,\n",
    "                    'jarque_bera_p': jb_p\n",
    "                },\n",
    "                'stationarity': {\n",
    "                    'adf_stat': adf_stat,\n",
    "                    'adf_p': adf_p,\n",
    "                    'kpss_stat': kpss_stat,\n",
    "                    'kpss_p': kpss_p\n",
    "                },\n",
    "                'heteroscedasticity': {\n",
    "                    'arch_lm_stat': lm_stat,\n",
    "                    'arch_lm_p': lm_p,\n",
    "                    'arch_f_stat': f_stat,\n",
    "                    'arch_f_p': f_p\n",
    "                },\n",
    "                'extremes': {\n",
    "                    'extreme_count': np.sum(extremes),\n",
    "                    'extreme_ratio': np.mean(extremes),\n",
    "                    **evt_metrics\n",
    "                },\n",
    "                'autocorrelation': {\n",
    "                    'lag1': acf[1] if len(acf) > 1 else 0,\n",
    "                    'lag5': acf[5] if len(acf) > 5 else 0\n",
    "                }\n",
    "            }\n",
    "\n",
    "            execution_time = time.time() - start_time\n",
    "            logger.debug(f\"Statistical tests for {symbol} completed in {execution_time:.4f}s\")\n",
    "\n",
    "            if self.profiling_enabled:\n",
    "                self.profile.disable()\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error running statistical tests for {symbol}: {e}\")\n",
    "            if self.profiling_enabled:\n",
    "                self.profile.disable()\n",
    "            return {}\n",
    "\n",
    "    def _adf_test(self, series: np.array) -> Tuple[float, float, Dict]:\n",
    "        \"\"\"Augmented Dickey-Fuller test for stationarity\"\"\"\n",
    "        result = adfuller(series)\n",
    "        return result[0], result[1], result[4]\n",
    "\n",
    "    def _autocorrelation(self, series: np.array, nlags: int = 10) -> np.array:\n",
    "        \"\"\"Calculate autocorrelation function with improved error handling\"\"\"\n",
    "        try:\n",
    "            if len(series) <= nlags + 1:\n",
    "                logger.warning(f\"Series length {len(series)} is too short for {nlags} lags\")\n",
    "                return np.array([1.0] + [0.0] * min(nlags, len(series) - 1))\n",
    "\n",
    "            # Remove NaN values\n",
    "            series = series[~np.isnan(series)]\n",
    "\n",
    "            # Calculate autocorrelations with error handling\n",
    "            acf = [1.0]  # lag 0 autocorrelation is always 1\n",
    "            for i in range(1, min(len(series), nlags + 1)):\n",
    "                if len(series) <= i:\n",
    "                    acf.append(0.0)\n",
    "                    continue\n",
    "\n",
    "                # Calculate correlation coefficient safely\n",
    "                try:\n",
    "                    corr_matrix = np.corrcoef(series[:-i], series[i:])\n",
    "                    if corr_matrix.size >= 4:  # 2x2 matrix\n",
    "                        acf.append(corr_matrix[0, 1])\n",
    "                    else:\n",
    "                        acf.append(0.0)\n",
    "                except (ValueError, RuntimeWarning) as e:\n",
    "                    logger.warning(f\"Error calculating autocorrelation at lag {i}: {e}\")\n",
    "                    acf.append(0.0)\n",
    "\n",
    "            return np.array(acf)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in autocorrelation calculation: {e}\")\n",
    "            return np.array([1.0] + [0.0] * nlags)\n",
    "\n",
    "    def _extract_iv_series(self, options_data: Dict) -> np.array:\n",
    "        \"\"\"Extract IV series from options data\"\"\"\n",
    "        iv_values = []\n",
    "        for expiry in options_data:\n",
    "            calls = options_data[expiry]['calls']\n",
    "            puts = options_data[expiry]['puts']\n",
    "            iv_values.extend(calls['impliedVolatility'].dropna())\n",
    "            iv_values.extend(puts['impliedVolatility'].dropna())\n",
    "        return np.array(iv_values)\n",
    "\n",
    "    def calculate_iv_crush_probability_enhanced(self, symbol: str) -> Dict:\n",
    "        \"\"\"Enhanced probability calculation with statistical tests\"\"\"\n",
    "        base_prob = self.calculate_iv_crush_probability(symbol)\n",
    "        if not base_prob:\n",
    "            return {}\n",
    "\n",
    "        options_data = self.fetch_options_data(symbol)\n",
    "        stat_tests = self.run_statistical_tests(symbol, options_data)\n",
    "\n",
    "        # Adjust probability based on statistical tests\n",
    "        prob_adjustments = self._calculate_probability_adjustments(stat_tests)\n",
    "\n",
    "        final_prob = base_prob['iv_crush_probability'] * (1 + prob_adjustments)\n",
    "        final_prob = min(max(final_prob, 0), 1)  # Ensure probability is between 0 and 1\n",
    "\n",
    "        # Include both base metrics and enhanced statistical metrics\n",
    "        result = {\n",
    "            'symbol': symbol,\n",
    "            'iv_crush_probability': final_prob,\n",
    "            'detail_scores': base_prob['detail_scores'],  # Keep original scores\n",
    "            'statistical_scores': {  # Add new statistical scores\n",
    "                'normality_score': 1.0 - stat_tests['normality']['shapiro_p'],\n",
    "                'stationarity_score': 1.0 - stat_tests['stationarity']['adf_p'],\n",
    "                'extreme_value_score': stat_tests['extremes']['extreme_ratio'],\n",
    "                'autocorrelation_score': abs(stat_tests['autocorrelation']['lag1'])\n",
    "            },\n",
    "            'statistical_tests': stat_tests,\n",
    "            'probability_adjustments': prob_adjustments\n",
    "        }\n",
    "\n",
    "        # Merge base and statistical scores\n",
    "        result['detail_scores'].update(result['statistical_scores'])\n",
    "        return result\n",
    "\n",
    "    def _calculate_probability_adjustments(self, stat_tests: Dict) -> float:\n",
    "        \"\"\"Calculate probability adjustments based on statistical tests and EVT metrics.\n",
    "\n",
    "        This method implements a comprehensive scoring system that considers multiple\n",
    "        statistical indicators to adjust the base IV crush probability. The scoring\n",
    "        system uses the following components and weights:\n",
    "\n",
    "        Weights:\n",
    "        - Normality (20%): How well the IV distribution fits a normal distribution\n",
    "        - Stationarity (20%): Whether the IV series shows mean-reverting behavior\n",
    "        - Heteroscedasticity (15%): Presence of volatility clustering\n",
    "        - Extremes (30%): Tail risk and extreme value behavior\n",
    "        - Autocorrelation (15%): Serial correlation in IV changes\n",
    "\n",
    "        Adjustment Logic:\n",
    "        1. Normality:\n",
    "           - Uses both Shapiro-Wilk and Jarque-Bera tests\n",
    "           - Higher adjustments for significant non-normality (p < 0.05)\n",
    "\n",
    "        2. Stationarity:\n",
    "           - Combines ADF and KPSS tests for robust stationarity assessment\n",
    "           - Higher adjustments for strong mean-reversion signals\n",
    "\n",
    "        3. Heteroscedasticity:\n",
    "           - Uses ARCH-LM test to detect volatility clustering\n",
    "           - Significant ARCH effects increase crush probability\n",
    "\n",
    "        4. Extreme Values:\n",
    "           - Considers both traditional outliers and EVT metrics\n",
    "           - VaR and ES measures contribute to tail risk assessment\n",
    "           - Higher adjustments for significant tail risks\n",
    "\n",
    "        5. Autocorrelation:\n",
    "           - Examines short-term (lag1) and medium-term (lag5) correlations\n",
    "           - Strong autocorrelation suggests persistent patterns\n",
    "\n",
    "        Returns:\n",
    "            float: The total probability adjustment factor (range: [0, 1])\n",
    "        \"\"\"\n",
    "        adjustments = 0\n",
    "        weights = {\n",
    "            'normality': 0.20,\n",
    "            'stationarity': 0.20,\n",
    "            'heteroscedasticity': 0.15,\n",
    "            'extremes': 0.30,  # Increased weight for extreme value analysis\n",
    "            'autocorrelation': 0.15\n",
    "        }\n",
    "\n",
    "        # 1. Normality Adjustments\n",
    "        if stat_tests.get('normality'):\n",
    "            shapiro_adj = 0.15 if stat_tests['normality']['shapiro_p'] < 0.05 else 0\n",
    "            jb_adj = 0.10 if stat_tests['normality']['jarque_bera_p'] < 0.05 else 0\n",
    "            adjustments += (shapiro_adj + jb_adj) * weights['normality']\n",
    "\n",
    "        # 2. Stationarity Adjustments\n",
    "        if stat_tests.get('stationarity'):\n",
    "            adf_adj = 0.15 if stat_tests['stationarity']['adf_p'] < 0.05 else 0\n",
    "            kpss_adj = 0.10 if stat_tests['stationarity']['kpss_p'] < 0.05 else 0\n",
    "            adjustments += (adf_adj + kpss_adj) * weights['stationarity']\n",
    "\n",
    "        # 3. Heteroscedasticity Adjustments\n",
    "        if stat_tests.get('heteroscedasticity'):\n",
    "            arch_adj = 0.20 if stat_tests['heteroscedasticity']['arch_lm_p'] < 0.05 else 0\n",
    "            adjustments += arch_adj * weights['heteroscedasticity']\n",
    "\n",
    "        # 4. Enhanced Extreme Values Adjustments\n",
    "        if stat_tests.get('extremes'):\n",
    "            extreme_ratio = stat_tests['extremes']['extreme_ratio']\n",
    "            var_99 = stat_tests['extremes']['var_99']\n",
    "            es_99 = stat_tests['extremes']['es_99']\n",
    "\n",
    "            # Assess tail risk using EVT metrics\n",
    "            tail_risk_adj = 0\n",
    "            if es_99 < var_99 * 1.5:  # Significant tail risk\n",
    "                tail_risk_adj += 0.20\n",
    "            if extreme_ratio > 0.15:  # More than 15% extreme values\n",
    "                tail_risk_adj += 0.15\n",
    "            elif extreme_ratio > 0.10:  # More than 10% extreme values\n",
    "                tail_risk_adj += 0.10\n",
    "\n",
    "            adjustments += tail_risk_adj * weights['extremes']\n",
    "\n",
    "        # 5. Autocorrelation Adjustments\n",
    "        if stat_tests.get('autocorrelation'):\n",
    "            lag1_acf = abs(stat_tests['autocorrelation']['lag1'])\n",
    "            if lag1_acf > 0.5:  # Strong autocorrelation\n",
    "                adjustments += 0.20 * weights['autocorrelation']\n",
    "            elif lag1_acf > 0.3:  # Moderate autocorrelation\n",
    "                adjustments += 0.10 * weights['autocorrelation']\n",
    "\n",
    "        return adjustments\n",
    "\n",
    "    def visualize_top_candidates(self, n_stocks: int = 10):\n",
    "        \"\"\"\n",
    "        Create visualizations for top IV crush candidates\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for symbol in self.sp500_symbols[:30]:  # Analyze first 30 to find top 10\n",
    "            result = self.calculate_iv_crush_probability_enhanced(symbol)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "\n",
    "        df = pd.DataFrame(results)\n",
    "        top_10 = df.nlargest(n_stocks, 'iv_crush_probability')\n",
    "\n",
    "        self._create_probability_distribution_plot(top_10)\n",
    "        self._create_component_scores_plot(top_10)\n",
    "        self._create_statistical_tests_plot(top_10)\n",
    "\n",
    "        return top_10\n",
    "\n",
    "    def _create_probability_distribution_plot(self, df: pd.DataFrame):\n",
    "        \"\"\"Create probability distribution plot\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Probability distribution\n",
    "        sns.kdeplot(data=df, x='iv_crush_probability', fill=True)\n",
    "\n",
    "        # Add individual points\n",
    "        plt.scatter(df['iv_crush_probability'], \n",
    "                   np.zeros_like(df['iv_crush_probability']),\n",
    "                   c='red', alpha=0.5)\n",
    "\n",
    "        # Add labels\n",
    "        for _, row in df.iterrows():\n",
    "            plt.annotate(row['symbol'], \n",
    "                        (row['iv_crush_probability'], 0.1),\n",
    "                        xytext=(0, 5), \n",
    "                        textcoords='offset points',\n",
    "                        ha='center')\n",
    "\n",
    "        plt.title('IV Crush Probability Distribution for Top Candidates')\n",
    "        plt.xlabel('Probability')\n",
    "        plt.ylabel('Density')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig('probability_distribution.png')\n",
    "        plt.close()\n",
    "\n",
    "    def _create_component_scores_plot(self, df: pd.DataFrame):\n",
    "        \"\"\"Create component scores comparison plot with enhanced metrics\"\"\"\n",
    "        plt.figure(figsize=(15, 8))\n",
    "\n",
    "        try:\n",
    "            # Extract and combine all scores\n",
    "            scores_data = []\n",
    "            for _, row in df.iterrows():\n",
    "                if isinstance(row.get('detail_scores'), dict):\n",
    "                    scores_data.append({\n",
    "                        'symbol': row['symbol'],\n",
    "                        **row['detail_scores']\n",
    "                    })\n",
    "\n",
    "            if scores_data:\n",
    "                scores_df = pd.DataFrame(scores_data)\n",
    "                if not scores_df.empty and len(scores_df.columns) > 1:\n",
    "                    scores_df.set_index('symbol', inplace=True)\n",
    "\n",
    "                    # Create heatmap\n",
    "                    sns.heatmap(scores_df, annot=True, cmap='YlOrRd', center=0.5, fmt='.2f')\n",
    "                    plt.title('Component Scores Comparison (Including Statistical Metrics)')\n",
    "                    plt.xlabel('Score Components')\n",
    "                    plt.ylabel('Symbols')\n",
    "                    plt.xticks(rotation=45)\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig('component_scores.png')\n",
    "                else:\n",
    "                    logger.warning(\"Insufficient data for component scores visualization\")\n",
    "            else:\n",
    "                logger.warning(\"No score data available for visualization\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating component scores plot: {e}\")\n",
    "            logger.exception(\"Full component scores plot error traceback:\")\n",
    "        finally:\n",
    "            plt.close()\n",
    "\n",
    "    def _create_statistical_tests_plot(self, df: pd.DataFrame):\n",
    "        \"\"\"Create enhanced statistical tests visualization with EVT metrics\"\"\"\n",
    "        plt.figure(figsize=(15, 10))\n",
    "\n",
    "        # Extract relevant statistical metrics including EVT\n",
    "        stats_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            if 'statistical_tests' in row:\n",
    "                stats = row['statistical_tests']\n",
    "                stats_data.append({\n",
    "                    'symbol': row['symbol'],\n",
    "                    'Normality (p)': stats['normality']['shapiro_p'],\n",
    "                    'Stationarity (p)': stats['stationarity']['adf_p'],\n",
    "                    'ARCH-LM (p)': stats['heteroscedasticity']['arch_lm_p'],\n",
    "                    'VaR 99%': stats['extremes']['var_99'],\n",
    "                    'ES 99%': stats['extremes']['es_99'],\n",
    "                    'ACF(1)': stats['autocorrelation']['lag1']\n",
    "                })\n",
    "\n",
    "        stats_df = pd.DataFrame(stats_data)\n",
    "        if not stats_df.empty:\n",
    "            stats_df = stats_df.set_index('symbol')\n",
    "\n",
    "            # Create enhanced heatmap\n",
    "            sns.heatmap(stats_df, annot=True, cmap='coolwarm', center=0.5, fmt='.3f',\n",
    "                       cbar_kws={'label': 'Standardized Values'})\n",
    "            plt.title('Statistical Tests Results (Including EVT Metrics)')\n",
    "            plt.xlabel('Statistical Metrics')\n",
    "            plt.ylabel('Symbols')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('statistical_tests.png')\n",
    "        plt.close()\n",
    "\n",
    "def main():\n",
    "    logger.info(\"Starting IV Crush Analysis...\")\n",
    "    analyzer = IVCrushAnalyzerEnhanced()\n",
    "\n",
    "    # Enable profiling (optional)\n",
    "    # analyzer.enable_profiling()\n",
    "\n",
    "    logger.info(\"Analyzing S&P 500 stocks for IV crush probability...\")\n",
    "    top_candidates = analyzer.visualize_top_candidates(10)\n",
    "\n",
    "    # Disable profiling (optional) and print stats\n",
    "    #analyzer.disable_profiling()\n",
    "\n",
    "    logger.info(\"\\nTop 10 IV Crush Candidates:\")\n",
    "    for _, row in top_candidates.iterrows():\n",
    "        logger.info(f\"\\nSymbol: {row['symbol']}\")\n",
    "        logger.info(f\"IV Crush Probability: {row['iv_crush_probability']:.2%}\")\n",
    "        logger.info(\"Statistical Tests:\")\n",
    "        for test_name, test_results in row['statistical_tests'].items():\n",
    "            logger.info(f\"  {test_name}:\")\n",
    "            for metric, value in test_results.items():\n",
    "                # Handle numpy array values\n",
    "                if isinstance(value, np.ndarray):\n",
    "                    formatted_value = \", \".join(f\"{v:.4f}\" for v in value)\n",
    "                    logger.info(f\"    {metric}: [{formatted_value}]\")\n",
    "                else:\n",
    "                    try:\n",
    "                        logger.info(f\"    {metric}: {float(value):.4f}\")\n",
    "                    except (TypeError, ValueError):\n",
    "                        logger.info(f\"    {metric}: {value}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
